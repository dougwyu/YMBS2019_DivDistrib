---
title: "Sichuan2014_metabarcode pipeline - simplified"
author: "Douglas Yu and Xiaoyang Wang"
date: "29/09/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Install packages
```{r install packages if needed, eval=FALSE, include=FALSE}
install.packages(c("ade4", "ape", "beanplot", "beepr", "betapart", "BiocManager", "boral", "car", "conflicted", "corrplot", "data.table", "iNEXT", "iNextPD", "metacoder", "mvabund", "phyloseq", "RColorBrewer", "rmarkdown", "SpeciesMix", "sjmisc", "tidyverse", "UpSetR", "vegan", "zetadiv"), dependencies = TRUE)

library(BiocManager)
BiocManager::install(c("GenomicRanges", "Biobase", "IRanges", "AnnotationDbi", "phyloseq", "GenomicAlignments", "Organism.dplyr")) # install Bioconductor packages

library(devtools)
install_github("tobiasgf/lulu")
```

# start R analysis
```{r setup, load packages}
library(ape) #read.tree() 
library(tidyverse)
library(vegan)
library(beanplot)
library(car)
library(iNEXT)
library(iNextPD)
library(ade4)
library(boral)
library(mvabund)
library(RColorBrewer)
library(betapart)
library(SpeciesMix)
library(beepr)
library(corrplot)
library(lulu)
library(phyloseq)
library(data.table)
library(metacoder)
library(UpSetR)
library(sjmisc)
library(conflicted)
    conflict_prefer("filter", "dplyr")
    conflict_prefer("select", "dplyr")
    # R will prefer dplyr::filter and select over any other package
    # filter and select are verbs used by multiple packages
sessionInfo()
```

#After CROP clustering with 97% similarity
#run following command on terminal for getting match_list
vsearch --usearch_global 2libs_CROP97.cluster3507.fasta --db 2libs_CROP97.cluster3507.fasta --self --id .84 --iddef 1 --userout match_list.txt -userfields query+target+id --maxaccepts 0 --query_cov .9 --maxhits 10

# lulu OTU collapsing
```{r code for lulu, eval=FALSE, include=FALSE}
comLuLu <- read.table("lulu_3507/2libs_CROP97_merge_otu3507.txt", header= T, sep = "\t") # OTU table, 3507 OTUs
comLuLu <- column_to_rownames(comLuLu, var = "Cluster")
View(comLuLu)

matchlist <- read.table("lulu_3507/match_list.txt", header=FALSE,as.is=TRUE, stringsAsFactors=FALSE)  # use vsearch or usearch or blast to compare every OTU with every OTU and record the OTU pairs that have "high" similarity. Look at column v3 (similarity)

curated_result <- lulu(comLuLu, matchlist) # default parameters:
# curated_result <- lulu(comLuLu, matchlist, minimum_ratio_type = "min", minimum_ratio = 1, minimum_match = 84, minimum_relative_cooccurence = 0.95)

new_table_lulu <- curated_result$curated_table
# 3507 collapsed to 1507 OTUs
```

# We did not use the DAMe/Begum pipeline, so we still have many false OTUs. From experience, we know that small OTUs are morelikely to be false OTUs. But what is small?  One possible method is that we use a heuristic method from this phyloseq tutorial:
http://evomics.org/wp-content/uploads/2016/01/phyloseq-Lab-01-Answers.html

First, just run this code chunk and try to understand the logic. Later, you can try to understand the code. 
```{r phyloseq, eval=FALSE, include=FALSE}
########## filter out small OTUs, phyloseq ##
## code from 
communityAll_t <- new_table_lulu
communityAll <- rotate_df(communityAll_t) # sjmisc
# communityAll <- t(communityAll_t) # base R:  t() transpose a table
TotalCounts <- c(colSums(communityAll)) 
TotalCounts # number of reads per OTU ("Cluster") (summed over all samples)

tdt = data.table(colnames(communityAll), TotalCounts = colSums(communityAll), OTU = colnames(communityAll)) # data.table() package

# histogram of OTU sizes.  You can 
ggplot(tdt, aes(TotalCounts)) + 
  geom_histogram() + 
  ggtitle("Histogram of Total Counts")

tdt[(TotalCounts <= 0), .N] # no OTUs with 0 reads
tdt[(TotalCounts <= 1), .N] # 315 OTUs with 1 read (single-read OTUs)
tdt[(TotalCounts <= 2), .N] # 410 OTUs with <=2 reads (two-read OTUs)
tdt[(TotalCounts <= 3), .N] # 476 OTUs with <=3 reads (two-read OTUs)

# these are data.table commands, and I am only following the phyloseq tutorial
taxcumsum = tdt[, .N, by = TotalCounts]
setkey(taxcumsum, TotalCounts)
taxcumsum[, CumSum := cumsum(N)]
# Define the plot
pCumSum = ggplot(taxcumsum, aes(TotalCounts, CumSum)) + 
  geom_point() +
  xlab("Filtering Threshold, Minimum Total Counts") +
  ylab("OTUs Filtered") +
  ggtitle("OTUs that would be filtered vs. the minimum count threshold")
pCumSum # this is the ggplot2 object
pCumSum + xlim(0, 200)

## phyloseq-class experiment-level object
## otu_table()   OTU Table:         [ 3426 taxa and 34 samples ]
## sample_data() Sample Data:       [ 34 samples by 11 sample variables ]
## tax_table()   Taxonomy Table:    [ 3426 taxa by 7 taxonomic ranks ]
## phy_tree()    Phylogenetic Tree: [ 3426 tips and 3424 internal nodes ]

# after viewing the above figure, we chose 44 as the minimum OTU size
# each column is an OTU. We sum the reads per OTU and only keep the columns (OTUs) that have >= 44 reads
commMBC.44 <- communityAll[ ,colSums(communityAll)>=44] # 594 otus remain
rowSums(commMBC.44) # the total number of reads (summed over all OTUs) per sample (row)
commMBC.44 <- commMBC.44[rowSums(commMBC.44)>0, ] # for safety, remove any samples that have no OTUs (this doesn't happen in this dataset, but it can happen when you remove a lot of OTUs)
rowSums(commMBC.44)
commMBC44copies <- rotate_df(commMBC.44) # transpose to new table (OTUs in rows, samples in columns)
# commMBC44copies <- t(commMBC.44) 

# save the new reduced dataset
write.table(commMBC44copies, file = "lulu_3507/2014MBC_44reads_otu594.txt", sep = "\t", row.names = TRUE, col.names = TRUE)
```

At this stage, we did some further filtering outside of R. 

32 OTUs removed after translating to amino acids and finding that they have stop codons (possible NUMTs). 
18 OTUs removed because they were assigned to non-Arthropoda taxa (using Naive Bayesian Classifier)
1 OTU removed because after BLASTing to an Arthropoda reference database (our own), it had a low similarity (so probably not Arthropoda)

This left us with 543 OTUs, and the new OTU table is stored inside the table 2014MBC_44reads_otu543_LandsatEnv.txt. This table *also* has all the environment metadata per sample. 

This is a safe way to store environment and community data together. Why is this safe? What is the danger?

## commnunity analyses start from here
```{r load and format data for community analyses}
inputfile <- "./data/2014MBC_44reads_otu543_LandsatEnv.txt" # post phyloseq filtering and filtering for trees, using final bioinformatics pipeline with vsearch and RDP Classifier and phyloseq at min20reads, with landsat data

# read table command from readr package, with options on formatting the columns
gfgMB <- read_tsv(
   inputfile, col_names = TRUE, na = "NA",
   col_types = cols(
     Site = col_character(),
     Habitat = col_factor(c("BB", "CL", "EC", "JC", "MP", "NF")),
     Type = col_factor(c("1", "2", "3", "4", "5", "6")),
     Altitude = col_integer(),
     sampling_time = col_date(format = "%d/%m/%Y"),
     weather_value = col_factor(c("cloudy", "sunny", "rainy")),
     Landsat_value = col_factor(c("1", "2", "3", "4", "5", "6")),
     longitude = col_double(),
     latitude = col_double(),
     Elevation_m = col_integer()
     )
 )

gfgMB <- as_tibble(gfgMB) # formats to 'tibble' format, which is more convenient.  It is still a data frame!
gfgMB

View(gfgMB) # to see the Cluster columns (the OTUs), look at the top of the window, and you will see "Cols" and some arrowheads. Click on them to scroll right and left.
```

```{r environment dataset}
# to extract the environment metadata into a pure environment dataset:  habitat
habitat <- gfgMB %>% 
  select(Site:Elevation_m, Simpson_evenness_index)
```

```{r community dataset}
# to extract the community dataset
community <- gfgMB %>% 
  select(starts_with("Cluster")) # dplyr::select() is smart!
communityAll <- gfgMB %>% 
  select(Site) %>% 
  bind_cols(community) %>% 
  column_to_rownames(var = "Site")

# alternative method 1
# community_t <- t(community)  
# community_t <- as.data.frame(community_t)
# community_t <- rownames_to_column(community_t)
# colnames(community_t) <- c("otu", gfgMB$Site) # add column names. This is the OTU X sample table, which is the bioinformatic format
# communityAll <- rotate_df(community_t, cn = TRUE) # sjmisc::rotate_df() to create the sample X OTU table, which is the format used in community statistics

# alternative method 2
# communityAll <- t(community_t) # this is sample X OTU table, which is the format used in community statistics, BUT the OTUs are in the first row, and it should be in the 
# colvector <- communityAll[1,] # make a vector of the first row, which has the otu names
# communityAll <- as.data.frame(communityAll)
# colnames(communityAll) <-  colvector # add the otu names to the column names
# communityAll <- communityAll[-1,] # remove first row, which has the column names
# # convert the columns to numeric from factor
# # http://stackoverflow.com/questions/2288485/how-to-convert-a-data-frame-column-to-numeric-type
# communityAll <- sapply(communityAll, function(x) as.numeric(as.character(x))) # sapply applies a function to each column, and the function is:  function(x) as.numeric(as.character(x)).  Cannot convert factors to numeric directly. first convert to character, then to numeric
# communityAll <- as.data.frame(communityAll) # then convert to df
```

# visualise a histogram of read number per OTU
```{r histogram of otu read numbers}
##### calculate distribution of read numbers per OTU to set minimum number 
otureads <- c(colSums(communityAll)) # list of the reads per OTU
sum(otureads) ## 1,900,539 reads total 
otureads[otureads>5000] <- 5000 # to make the histogram readable
otuhist <- hist(otureads, breaks = 100)
text(otuhist$mids, otuhist$counts, cex = 0.5, otuhist$counts, adj = c(.5, -.5), col = "blue3")
# notice that we have removed the small OTUs.  
```

# Starting ecological analysis
Filter out *sites* with (1) low reads (<= 100) and/or (2) very low numbers of species
```{r filter out sites }
community <- communityAll
#community <- t(new_table_lulu)
# community <- as.data.frame(community)
#sort(colSums(community))
community[community < 5] <- 0 # If an OTU has <5 reads *in a site*, remove it, because the detection is less reliable

habitat$rowsums <- rowSums(community) # add total OTU read number to the environment dataset
habitat$sprichness <- vegan::specnumber(community, MARGIN = 1) # number of species per site

# keep only sites that have more than 100 reads (removed 2 sites, 68 remain)
community <- community %>% dplyr::filter(habitat$rowsums > 100)
habitatN <- habitat %>% dplyr::filter(habitat$rowsums > 100)
rowSums(community) 

# keep only sites that have >=5 species (61 sites remain from original 68 sites)
community <- community %>% dplyr::filter(habitatN$sprichness >= 5)
habitatN <- habitatN %>% dplyr::filter(habitatN$sprichness >= 5) # also drop the sites from the environment dataset
#sort(colSums(community))
habitatN <- droplevels(habitatN)
# after removing some sites, it is possible that some OTUs become very small (their reads were (mostly) present in the sites that were removed). So we remove the OTUs that have been depleted to < 20 reads in total (this is a bit arbitrary)
community <- community[, colSums(community)>=20] # 7 OTUs removed, leaving 536 OTUs
#Cluster31064  Cluster334816  Cluster672922  Cluster932857  Cluster989447  Cluster273565 Cluster1265861
```

We now have our (almost) final dataset. We can start doing analyses.

```{r beanplot of read number by habitat}
beanplot(rowSums(community)~habitatN$Habitat, col = c("grey", "white"), xlab = "Habitat type", ylab = "Reads number") # observed number of OTUs per habitat type
# Kampstra, P. Beanplot: A Boxplot Alternative for Visual Comparison of Distributions. Journal of Statistical Software, Code Snippets 28(1). 1-9 (2008) 

# We want to see if the number of reads (the amount of raw data) is roughly equal per habitat (because we are going to compare species richness per habitat, and thus we want to have roughly the same amount of raw data per habitat)

# this is a cool-kids way of doing ANOVA, but using the mvabund package. We just want to 
nboot <- 999 # set to 999 for publication
# base model with no levels combined
reads.glm <- mvabund::manyglm(rowSums(community) ~ habitatN$Habitat)
plot(reads.glm)
anova(reads.glm, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.036, df = 5, score = 8.671, nBoot = 999, 1 min. The habitats do not have different numbers of reads.
```

# Metacoder heat trees. Fewer OTUs because we have filtered out some sites with low total number of reads (low data) and/or low total number of species. Now 536 OTUs. 
```{r metacoder, warning=FALSE}
mbc_otus <- read.table("data/forMetacoder/2014MBC_otu536_tax.txt", header = T, sep = "\t")
mbc_samples <- read.table("data/forMetacoder/2014MBC_sample536.txt", header = T, sep = "\t")
View(mbc_otus) # notice the taxonomy column (from RDP Classifier) in the OTU table
View(mbc_samples) # environment metadata table

mbc_otus[mbc_otus>1] <- 1 # change to presence/absence data

#str(mbc_otus)
#str(mbc_samples)
## change data type 
mbc_otus$OTU_id <- as.character(mbc_otus$OTU_id)
mbc_otus$lineage <- as.character(mbc_otus$lineage)
mbc_samples$Site <- as.character(mbc_samples$Site)
mbc_samples$Habitat <- as.character(mbc_samples$Habitat)
##
mbc_otus <- as_tibble(mbc_otus)
mbc_samples <- as_tibble(mbc_samples)
#print(mbc_otus)
#print(mbc_samples)

# metacoder code
# This code comes directly from the metacoder tutorial
##### first remove CL group to see differences in forest and plantations only 
otus_without_CL <- mbc_otus %>% dplyr::select(-c(CL01:CL16))
samples_without_CL <- mbc_samples %>% dplyr::filter(mbc_samples$Habitat %in% c("BB", "EC", "JC", "MP", "NF"))

print(otus_without_CL)
print(samples_without_CL)
#str(otus_without_CL)
#str(samples_without_CL)
objF <- parse_tax_data(otus_without_CL, class_cols = "lineage", class_sep = ";",
                      class_key = c(tax_rank = "info", tax_name = "taxon_name"),
                      class_regex = "^(.+)__(.+)$")

# This returns a taxmap object. The taxmap class is designed to store any number of tables, lists, or vectors associated with taxonomic information and facilitate manipulating the data in a cohesive way. Here is what that object looks like:

print(objF) # or click on the object in the Environment pane

# removing low abundance counts
#objF$data$tax_data <- zero_low_counts(objF, "tax_data", min_count = 5)
#no_reads <- rowSums(objF$data$tax_data[, samples_without_CL$Site]) == 0
#sum(no_reads)
#objF <- filter_obs(objF, "tax_data", ! no_reads, drop_taxa = TRUE)
#print(objF)

# accounting for un-even sampling
objF$data$tax_data <- calc_obs_props(objF, "tax_data")

print(objF)

# Getting per-taxon information
# Currently, we have values for the abundance of each OTU, not each taxon. To get information on the taxa, we can sum the abundance per-taxon like so:
objF$data$tax_abund <- calc_taxon_abund(objF, "tax_data",
                                       cols = samples_without_CL$Site)
print(objF)
# Note that there is now an additional table with one row per taxon.
# We can also easily calculate the number of samples that have reads for each taxon:
objF$data$tax_occ <- calc_n_samples(objF, "tax_abund", groups = samples_without_CL$Habitat)
print(objF)

# Plotting taxonomic data
# Now that we have per-taxon information, we can plot the information using heat trees. The code below plots the number of “Nose” samples that have reads for each taxon. It also plots the number of OTUs assigned to each taxon in the overall dataset.

heat_tree(objF, 
          node_label = objF$taxon_names(),
          node_size = objF$n_obs(),
          node_color = objF$data$tax_occ$NF, 
          node_size_axis_label = "OTU count",
          node_color_axis_label = "Log2 ratio median proportions")

# Pairwise comparisons of all the habitats
objF$data$diff_table <- compare_groups(objF, dataset = "tax_abund",
                                      cols = samples_without_CL$Site,
                                      groups = samples_without_CL$Habitat)
print(objF$data$diff_table)
heat_tree_matrix(objF,
                 data = "diff_table",
                 node_size = n_obs, # number of OTUs with that taxon
                 node_label = taxon_names,
                 node_color = log2_median_ratio,
                 node_color_range = diverging_palette(),
                 node_color_trans = "linear",
                 node_color_interval = c(-3, 3),
                 edge_color_interval = c(-3, 3),
                 node_size_axis_label = "Number of OTUs",
                 node_color_axis_label = "Log2 ratio median proportions")
```

DO NOT RUN THIS CHUNK.  This is the code for metacoder heat trees that include the CropLand (CL) samples
```{r metacoder, eval=FALSE, include=FALSE}
obj <- parse_tax_data(mbc_otus, class_cols = "lineage", class_sep = ";",
                      class_key = c(tax_rank = "info", tax_name = "taxon_name"),
                      class_regex = "^(.+)__(.+)$")
# This returns a taxmap object. The taxmap class is designed to store any number of tables, lists, or vectors associated with taxonomic information and facilitate manipulating the data in a cohesive way. Here is what that object looks like:

print(obj) # or click on the object in the Environment pane

# accounting for un-even sampling
obj$data$tax_data <- calc_obs_props(obj, "tax_data")

print(obj)

# Getting per-taxon information
# Currently, we have values for the abundance of each OTU, not each taxon. To get information on the taxa, we can sum the abundance per-taxon like so:
obj$data$tax_abund <- calc_taxon_abund(obj, "tax_data",
                                       cols = mbc_samples$Site)
print(obj)
# Note that there is now an additional table with one row per taxon.
# We can also easily calculate the number of samples have reads for each taxon:
obj$data$tax_occ <- calc_n_samples(obj, "tax_abund", groups = mbc_samples$Habitat)
print(obj)

# Plotting taxonomic data
# Now that we have per-taxon information, we can plot the information using heat trees. The code below plots the number of “Nose” samples that have reads for each taxon. It also plots the number of OTUs assigned to each taxon in the overall dataset.

# takes a minute, and the plot window must be big
heat_tree(obj, 
          node_label = obj$taxon_names(),
          node_size = obj$n_obs(),
          node_color = obj$data$tax_occ$NF, 
          node_size_axis_label = "OTU count",
          node_color_axis_label = "Samples with reads")

# Comparing any number of treatments/groups (we compare the different habitats to each other)
obj$data$diff_table <- compare_groups(obj, dataset = "tax_abund",
                                      cols = mbc_samples$Site,
                                      groups = mbc_samples$Habitat)
print(obj$data$diff_table) # print the output table
# now make the pairwise comparison heat trees
heat_tree_matrix(obj,
                 data = "diff_table",
                 node_size = n_obs,
                 node_label = taxon_names,
                 node_color = log2_median_ratio,
                 node_color_range = diverging_palette(),
                 node_color_trans = "linear",
                 node_color_interval = c(-3, 3),
                 edge_color_interval = c(-3, 3),
                 node_size_axis_label = "Number of OTUs",
                 node_color_axis_label = "Log2 ratio median proportions")

# page(compare_groups) #check codes of compare_groups, change "median" to "mean"


```


# Alpha Diversity
```{r make presence/absence dataset}
communityB <- community # B for binary
communityB[communityB>1] <- 1 # binary
rownames(communityB) # 61 rows = sites
```

```{r beanplot}
beanplot(specnumber(communityB)~habitatN$Habitat, col = c("grey", "white"), ylab = "Species richness") # numbers of observed species per site. Observed species are an underestimate of the true species richness, so now we estimate how much true species richness there is per habitat
```

# traditional Chao2 estimate for species richness
```{r specpool, }
######## otu table with original reads number
(pool1 <- vegan::specpool(communityB, habitatN$Habitat))
```

   Species     chao  chao.se     jack1 jack1.se    jack2     boot   boot.se  n
BB      83 185.9796 39.89775 132.71429 20.90210 165.8095 104.1117  8.989364  7
CL     194 336.1658 37.44452 295.73333 32.32777 358.8143 238.0445 15.679575 15
EC      64 115.4592 22.33722  99.14286 16.06619 120.0952  79.3403  7.646678  7
JC      85 209.6154 48.52236 139.00000 23.08246 177.7556 107.5053 10.816630 10
MP     119 405.5079 98.01090 203.44444 33.92075 267.8056 153.4993 14.358124  9
NF     210 507.3394 72.71267 346.61538 48.44615 445.4744 266.7256 22.180423 13

```{r barchart for Chao2 richness estimates}
es_value <- pool1$chao # from the chao2 output file
se <- pool1$chao.se # from the chao2 output file
# es_value <- c(185.9796, 115.4592, 209.6154, 405.5079, 336.1658, 507.3394)
# se <- c(39.89775, 22.33722, 48.52236, 98.01090, 37.44452, 72.71267)

# function to generate error bar limits for the barchart
error.bar <- function(x, y, upper, lower=upper, length=0.1,...){
  if(length(x) != length(y) | length(y) !=length(lower) | length(lower) != length(upper))
    stop("vectors must be same length")
  arrows(x,y+upper, x, y-lower, angle=90, code=3, length=length, ...)
}

par(mfrow=c(1,1))
bar_es <- barplot(es_value, names.arg = c("BB", "CL", "EC", "JC", "MP", "NF"), col = "lightblue", ylim = c(0, 600), border = NA, ylab = "Species richness estimates")
error.bar(bar_es, es_value, se)
par(mfrow=c(1,1))
```

```{r Welch-t test for chao2 results}
############ This function (t.test2) will calculate Welch's test (t-test when you only have means (m) and standard deviations (s))
t.test2 <- function(m1, m2, s1, s2, n1, n2, m0=0, equal.variance=FALSE)
{
  if( equal.variance==FALSE ) 
  {
    se <- sqrt( (s1^2/n1) + (s2^2/n2) )
    # welch-satterthwaite df
    df <- ( (s1^2/n1 + s2^2/n2)^2 )/( (s1^2/n1)^2/(n1-1) + (s2^2/n2)^2/(n2-1) )
  } else
  {
    # pooled standard deviation, scaled by the sample sizes
    se <- sqrt( (1/n1 + 1/n2) * ((n1-1)*s1^2 + (n2-1)*s2^2)/(n1+n2-2) ) 
    df <- n1+n2-2
  }      
  t <- (m1-m2-m0)/se 
  dat <- c(m1-m2, se, t, round(df,1), 2*pt(-abs(t),df))    
  # names(dat) <- c("Difference of means", "Std Error", "t", "df", "p-value")
  return(dat) 
}

vec <- rep(NA, 15)

# t test for BB and CL
vec[1] <- t.test2(pool1[1, 2], pool1[2, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[2, 3]*sqrt(pool1[2, 9]), pool1[1, 9], pool1[2, 9])[5]

# t test for BB and EC
vec[2] <- t.test2(pool1[1, 2], pool1[3, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[3, 3]*sqrt(pool1[3, 9]), pool1[1, 9], pool1[3, 9])[5]

# t test for BB and JC
vec[3] <- t.test2(pool1[1, 2], pool1[4, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[4, 3]*sqrt(pool1[4, 9]), pool1[1, 9], pool1[4, 9])[5]

# t test for BB and MP
vec[4] <- t.test2(pool1[1, 2], pool1[5, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[5, 3]*sqrt(pool1[5, 9]), pool1[1, 9], pool1[5, 9])[5]

# t test for BB and NF
vec[5] <- t.test2(pool1[1, 2], pool1[6, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[1, 9], pool1[6, 9])[5]

# t test for CL and EC
vec[6] <- t.test2(pool1[2, 2], pool1[3, 2], pool1[2, 3]*sqrt(pool1[2, 9]), pool1[3, 3]*sqrt(pool1[3, 9]), pool1[2, 9], pool1[3, 9])[5]

# t test for CL and JC
vec[7] <- t.test2(pool1[2, 2], pool1[4, 2], pool1[2, 3]*sqrt(pool1[2, 9]), pool1[4, 3]*sqrt(pool1[4, 9]), pool1[2, 9], pool1[4, 9])[5]

# t test for CL and MP
vec[8] <- t.test2(pool1[2, 2], pool1[5, 2], pool1[2, 3]*sqrt(pool1[2, 9]), pool1[5, 3]*sqrt(pool1[5, 9]), pool1[2, 9], pool1[5, 9])[5]

# t test for CL and NF
vec[9] <- t.test2(pool1[2, 2], pool1[6, 2], pool1[2, 3]*sqrt(pool1[2, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[2, 9], pool1[6, 9])[5]

# t test for EC and JC
vec[10] <- t.test2(pool1[3, 2], pool1[4, 2], pool1[3, 3]*sqrt(pool1[3, 9]), pool1[4, 3]*sqrt(pool1[4, 9]), pool1[3, 9], pool1[4, 9])[5]

# t test for EC and MP
vec[11] <- t.test2(pool1[3, 2], pool1[5, 2], pool1[3, 3]*sqrt(pool1[3, 9]), pool1[5, 3]*sqrt(pool1[5, 9]), pool1[3, 9], pool1[5, 9])[5]

# t test for EC and NF
vec[12] <- t.test2(pool1[3, 2], pool1[6, 2], pool1[3, 3]*sqrt(pool1[3, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[3, 9], pool1[6, 9])[5]

# t test for JC and MP
vec[13] <- t.test2(pool1[4, 2], pool1[5, 2], pool1[4, 3]*sqrt(pool1[4, 9]), pool1[5, 3]*sqrt(pool1[5, 9]), pool1[4, 9], pool1[5, 9])[5]

# t test for JC and NF
vec[14] <- t.test2(pool1[4, 2], pool1[6, 2], pool1[4, 3]*sqrt(pool1[4, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[4, 9], pool1[6, 9])[5]

# t test for MP and NF
vec[15] <- t.test2(pool1[5, 2], pool1[6, 2], pool1[5, 3]*sqrt(pool1[5, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[5, 9], pool1[6, 9])[5]

# BB_CL, BB_EC, BB_JC, BB_MP, BB_NF, CL_EC, CL_JC, CL_MP, CL_NF, EC_JC, EC_MP, EC_NF, JC_MP, JC_NF, MP_NF
p_values <- vec
p_values.corr.fdr<-p.adjust(p_values, method = "fdr", n = length(p_values)) 
p_values.corr.fdr
#  [1] 0.0432930174 0.1948572203 0.7119988942 0.1058460773 0.0059727485 0.0009117642 0.0996143684
#  [8] 0.5604343529 0.0996143684 0.1399044218 0.0459757611 0.0010730017 0.1399044218 0.0106130314
# [15] 0.4803785786

```

iNext analyses
```{r make community dataset for iNext}
# create separate community datasets for each habitat
BB <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("BB"))
CL <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("CL"))
EC <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("EC"))
JC <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("JC"))
MP <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("MP"))
NF <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("NF"))

cname <- c("BB","CL","EC","JC","MP","NF")

comm4inext_abun <- matrix(c(colSums(BB), colSums(CL), colSums(EC), colSums(JC), colSums(MP), colSums(NF)), ncol = 6) # BB, CL, EC, JC, MP, NF from 'r t-test for observed species richness'
colnames(comm4inext_abun) <- cname

colnameBB <- colnames(BB)
rownames(comm4inext_abun) <- colnameBB # add OTU names to the rownames

View(comm4inext_abun)

# add a first row that is the number of sites per habitat
comm4inext <- rbind(c(nrow(BB), nrow(CL), nrow(EC), nrow(JC), nrow(MP), nrow(NF)), comm4inext_abun) 
View(comm4inext)
# e.g. Cluster1003060 is observed in 2 of the 7 EC habitat samples
```

# Interpolation and extrapolation of Hill number 
```{r iNEXT}
# http://chao.stat.nthu.edu.tw/wordpress/wp-content/uploads/software/iNEXT_UserGuide.pdf

confnum=0.95 # set confidence here
outcomm0 <- iNEXT(comm4inext, q=0, conf=confnum, datatype="incidence_freq")
# Hill numbers (q):  0 = sp richness, 1 = Shannon, 2 = inverse Simpson
outcomm0$DataInfo
ChaoRichness(comm4inext, datatype="incidence_freq") # same as specpool results, so i trust that we have done this correctly
ChaoShannon(comm4inext, datatype="incidence_freq")

outI <- iNEXT(comm4inext, q=c(0,1,2), conf=confnum, datatype="incidence_freq")
# Sample‐size‐based R/E curves, separating by "site"
ggiNEXT(outI, type=1, facet.var="order") + theme_bw(base_size = 18)
```

But we do not fully trust our OTUs.  It is very possible that we have 'oversplit' (some of) our OTUs. How do we compare alpha diversity? One way is to use phylogenetic diversity:  how much total branch length is represented by each habitat (on average)?  If we use PD (phylogenetic diversity), then 'oversplit OTUs" should be near each other on the tree (different leaves on the same branch), and the multiple OTUs will add only a little additional branch length. 

Thus, we made a phylogenetic tree from our aligned OTUs, and we use iNextPD.
```{r iNextPD, warning=FALSE }
commPD <- communityB # sample X OTU table, binary
# commPD$Cluster418132 #
# commPD$Cluster636975 #
# commPD$Cluster549885 #
commPD$Cluster99619

#remove 3 OTUs with very long branches in the tree
commPD$Cluster418132 <- NULL
commPD$Cluster636975 <- NULL
commPD$Cluster549885 <- NULL

BB <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("BB"))
CL <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("CL"))
EC <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("EC"))
JC <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("JC"))
MP <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("MP"))
NF <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("NF"))
comm4inextPD <- matrix(c(colSums(BB), colSums(CL), colSums(EC), colSums(JC), colSums(MP), colSums(NF)), ncol = 6)

colnames(comm4inextPD) <- cname
colnameBB <- colnames(BB)
rownames(comm4inextPD) <- colnameBB

# this is the phylogeny of the OTUs (without the )
MLtree.tre <- read.table("./data/2014MBC_535otu-2collembola_align533.newick")
ML.tre <- ade4::newick2phylog(MLtree.tre$V1)
ML.lab <- rownames(comm4inextPD)

#rownames(comm4inext_abun)
BBnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("BB"))
CLnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("CL"))
ECnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("EC"))
JCnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("JC"))
MPnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("MP"))
NFnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("NF"))

rownames(BB) <- BBnames$Site
rownames(CL) <- CLnames$Site
rownames(EC) <- ECnames$Site
rownames(JC) <- JCnames$Site
rownames(MP) <- MPnames$Site
rownames(NF) <- NFnames$Site

BBnames$Site

#commB <- list(BB.Site = t(BB), CL.Site = t(CL), EC.Site = t(EC), JC.Site = t(JC), MP.Site = t(MP), NF.Site = t(NF))
commB <- list(BB = t(BB), CL = t(CL), EC = t(EC), JC = t(JC), MP = t(MP), NF = t(NF))


out <- iNextPD(commB, ML.lab, ML.tre, q=c(0, 1, 2), datatype="incidence_raw", endpoint = 30, se = TRUE)
# Sample‐size‐based R/E curves, separating by "site""
#ggiNEXT(out, type=1, facet.var="site") +theme_bw(base_size = 18)
# Sample‐size‐based R/E curves, separating by "order"
ggiNEXT(out, type=1, facet.var="order")+theme_bw(base_size = 18)
# display black‐white theme
#ggiNEXT(out, type=1, facet.var="order", grey=TRUE)

table.phylog(comm4inextPD, ML.tre, csize=2, f.phylog=0.7)

```

# Beta diversity turnover versus nestedness
## Run tabasco before removing zerotons and singletons
```{r tabasco}
community.jmds <- metaMDS(communityB, distance = "jaccard", trymax = 40, binary=TRUE)
community.jmds <- metaMDS(communityB, distance = "jaccard", binary = TRUE, previous.best = community.jmds)  # doesn't converge well, with final stress > 0.20
stressplot(community.jmds)
tabasco(community, use = community.jmds, labCol = habitatN$Habitat, col = brewer.pal(3, "Oranges"))
```

```{r UpSetR}
# In our case, Rows are OTUs, Columns are land-cover types (BB, EC, JC, MP, NF, CL). So this is pretty easy to make. 0/1 dataset
comm4inext_abun <- as.data.frame(comm4inext_abun) # comm4inext_abun from iNEXT
comm4inext_abun[comm4inext_abun>1] <- 1
colSums(comm4inext_abun)
# comm4inext_abun$MF <- comm4inext_abun$MP
#sort(rowSums(comm4inext_abun))
# upset(comm4inext_abun, nsets = 6, nintersects = 70, cutoff = 7, keep.order = TRUE)
#upset(comm4inext_abun, sets = c("NF", "MP", "BB", "EC", "JC", "CL"), nintersects = 70, group.by = "sets", cutoff = 7, keep.order = TRUE)
upset(comm4inext_abun, sets = c("NF", "MP", "BB", "EC", "JC", "CL"), cutoff = 6, nintersects = NA, group.by = "sets", keep.order = TRUE)
upset(comm4inext_abun, sets = c("NF", "MP", "BB", "EC", "JC", "CL"), nintersects = NA, group.by = "sets", keep.order = TRUE)
# upset(comm4inext_abun, sets = c("NF", "MF", "BB", "EC", "JC", "CL"), cutoff = 6, nintersects = NA, group.by = "sets", keep.order = TRUE)
# upset(comm4inext_abun, sets = c("NF", "MF", "BB", "EC", "JC", "CL"), nintersects = NA, group.by = "sets", keep.order = TRUE)

```

## Betapart analysis before removing zerotons and singletons
```{r betapart, warning=FALSE}
communityBbetapart <- bind_cols(habitatN, communityB) 
communityBbetapart <- communityBbetapart %>% dplyr::select(-c(Habitat:sprichness))

JCNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("JC", "NF")) %>% column_to_rownames(var="Site")
BBNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("BB", "NF")) %>% column_to_rownames(var="Site")
CLNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("CL", "NF")) %>% column_to_rownames(var="Site")
ECNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("EC", "NF")) %>% column_to_rownames(var="Site")
MPNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("MP", "NF")) %>% column_to_rownames(var="Site")

JCNF.multi.dist <- beta.multi(JCNF, index.family="jac")
BBNF.multi.dist <- beta.multi(BBNF, index.family="jac")
CLNF.multi.dist <- beta.multi(CLNF, index.family="jac")
ECNF.multi.dist <- beta.multi(ECNF, index.family="jac")
MPNF.multi.dist <- beta.multi(MPNF, index.family="jac")

multi.all <- list(JCNF = JCNF.multi.dist, BBNF = BBNF.multi.dist, CLNF = CLNF.multi.dist, ECNF =  ECNF.multi.dist, MPNF = MPNF.multi.dist)


ALL.dist <- communityBbetapart %>% column_to_rownames(var="Site") %>% beta.pair(index.family="jac")
ALL.dist.subset <- ALL.dist[["beta.jne"]]
ALL.dist.jne.jmds <- metaMDS(ALL.dist.subset)
ALL.dist.jne.jmds <- metaMDS(ALL.dist.subset, previous.best = ALL.dist.jne.jmds)
stressplot(ALL.dist.jne.jmds)
ALL.dist.subset <- ALL.dist[["beta.jtu"]]
ALL.dist.jtu.jmds <- metaMDS(ALL.dist.subset)
ALL.dist.jtu.jmds <- metaMDS(ALL.dist.subset, previous.best = ALL.dist.jne.jmds)
stressplot(ALL.dist.jtu.jmds)

par(mfrow=c(1,1))
# , ylim = c(-0.5, 0.5), xlim = c(-0.4, 0.4)
par(mfrow=c(2,2))
colvec <- brewer.pal(5, "Set1")
with(habitatN, ordisurf(community.jmds, sprichness, main="All beta diversity", cex=0.5, col = "white"),  ylim = c(-0.5, 0.5))
# plot(community.jmds, main = "All beta diversity", ylim = c(-0.4, 0.4))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("BB"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[1]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("CL"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("EC"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("JC"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("MP"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("NF"))))

plot(ALL.dist.jtu.jmds, main = "Turnover beta diversity only", ylim = c(-0.5, 0.5))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("BB"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[1]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("CL"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("EC"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("JC"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("MP"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("NF"))))
    
plot(ALL.dist.jne.jmds, main = "Nestedness beta diversity only", ylim = c(-0.5, 0.5))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("BB"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[1]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("CL"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("EC"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("JC"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("MP"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("NF"))))
    
par(mfrow=c(1,1))

```

# Total beta diversity NMDS is correlated with turnover-only beta diversity 
```{r}
protestobj <- protest(community.jmds, ALL.dist.jtu.jmds)
plot(protestobj)
```

Call:
protest(X = community.jmds, Y = ALL.dist.jtu.jmds) 

Procrustes Sum of Squares (m12 squared):        0.07381 
Correlation in a symmetric Procrustes rotation: 0.9624 
Significance:  0.001 

Permutation: free
Number of permutations: 999

Total beta diversity NMDS is not correlated with nestedness-only beta diversity
```{r}
protest(community.jmds, ALL.dist.jne.jmds)
```

Call:
protest(X = community.jmds, Y = ALL.dist.jne.jmds) 

Procrustes Sum of Squares (m12 squared):        0.9937 
Correlation in a symmetric Procrustes rotation: 0.07915 
Significance:  0.909 

Permutation: free
Number of permutations: 999

# Beta diversity UNCONSTRAINED ordination
############ IMPORTANT #####################################
## For NMDS, mvabund, and boral analyses, remove zerotons and singletons

```{r remove zerotons and singletons from communityB}
communityB <- communityB[, which(specnumber(communityB, MARGIN=2) > 1)]
# 267 species remained

```

## NMDS ordination
```{r NMDS}
### do NMDS analysis to quickly see patterns ####
community.jmds <- metaMDS(communityB, distance = "jaccard", trymax = 40, binary=TRUE)
community.jmds <- metaMDS(communityB, distance = "jaccard", binary = TRUE, previous.best = community.jmds)  # doesn't converge well, with final stress > 0.20
plot(community.jmds)
stressplot(community.jmds)
```

## boral
Tested different error families (testing code is archived down below).  Based on residuals, decided on family = "binomial," which is good because the dataset is presence/absence. Using row.effect = "random" to get a composition-only analysis.   Now rerun with many more iterations

# This takes a long time to run, should run overnight when using mcmc.control4 parameters
```{r boral high iterations}
# set up MCMC parameters
mcmc.control <- list(n.burnin = 300, n.iteration = 1000, n.thin = 30, seed = 123) # for debugging

# mcmc.control4 <- list(n.burnin = 10000, n.iteration = 40000, n.thin = 30) # for paper publishing

# Set up priors
# Francis Hui suggests trying different priors to stabilise sampling for sparse matrices (such as we have).  This isn't as important as getting the mcmc.control parameters right
#set.prior <- list(type = c("normal","normal","normal","uniform"), hypparams = c(10, 10, 10, 30), ssvs.index = ssvsindex)  
# type = c("normal","normal","normal","uniform"), hypparams = c(10, 10, 10, 30) # boral default
# type = c("cauchy","cauchy","cauchy","uniform"), hypparams = c(2.5^2, 2.5^2, 2.5^2, 30) # Gelman proposed this
# type = c("normal","normal","normal","uniform"), hypparams = c(1, 1, 1, 30) ## People who developed the STAN package proposed this as one possibility. 


# set up model
commB.fit.b3.4.none <- boral(communityB, family = "binomial", lv.control = list(num.lv = 2), row.eff = "random", mcmc.control = mcmc.control, save.model = TRUE); beep(sound = 1)

# save output in case i want to do other analyses on it later
# saveRDS(commB.fit.b3.4.none, "boral_commB.fit.b3.4.RDS")
# commB.fit.b3.4.none <- readRDS("boral_commB.fit.b3.4.RDS") # to restore

summary(commB.fit.b3.4.none)
par(mfrow = c(2,2))
plot(commB.fit.b3.4.none) ## Plots used in residual analysis, 
par(mfrow = c(1,1))

commB.fit.b3.4.none.ord <- lvsplot(commB.fit.b3.4.none, biplot=FALSE, col = as.numeric(habitatN$Habitat), return.vals = TRUE)

res.cors <- get.residual.cor(commB.fit.b3.4.none); beep(1) # residual deviation
res.cors$trace
# 4419.197
# habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,1]

cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,1], method = "pearson")
# t = -3.9509, df = 59, p-value = 0.0002106
# alternative hypothesis: true correlation is not equal to 0
# 95 percent confidence interval: -0.6359671 -0.2323392
# sample estimates: cor -0.4573986
cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,1], method = "kendall")
cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,1], method = "spearman")
# S = 56743, p-value = 4.022e-05
# alternative hypothesis: true rho is not equal to 0
# sample estimates: rho -0.5003503 

cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,2], method = "pearson")
# t = -5.0463, df = 59, p-value = 4.603e-06
# alternative hypothesis: true correlation is not equal to 0
# 95 percent confidence interval: -0.7036107 -0.3449532
#sample estimates: cor -0.5490776 

cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,2], method = "kendall")
cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,2], method = "spearman")
# S = 59369, p-value = 1.645e-06
# alternative hypothesis: true rho is not equal to 0
#sample estimates: rho -0.5697854 

```

Try to interpret the latent variables. Latent variables are clearly correlated with the forest types (habitatN$Habitat) but weakly at best correlated with Simpson_evenness_index, Elevation_m, and weather_value. 
*There does seem to be a quadratic relationship between lvs2 and Elevation_m*. *The correlation plot suggests that there are some informative Landsat channels.*  
```{r boral lvs correlations}
cbind(commB.fit.b3.4.none.ord$scaled.lvs,habitatN$Habitat)
par(mfrow = c(1,2))
plot(commB.fit.b3.4.none.ord$scaled.lvs[,1]~habitatN$Habitat)
plot(commB.fit.b3.4.none.ord$scaled.lvs[,2]~habitatN$Habitat)

plot(commB.fit.b3.4.none.ord$scaled.lvs[,1]~habitatN$Simpson_evenness_index)
plot(commB.fit.b3.4.none.ord$scaled.lvs[,2]~habitatN$Simpson_evenness_index)

plot(commB.fit.b3.4.none.ord$scaled.lvs[,1]~habitatN$Elevation_m)
plot(commB.fit.b3.4.none.ord$scaled.lvs[,2]~habitatN$Elevation_m)

plot(commB.fit.b3.4.none.ord$scaled.lvs[,1]~habitatN$weather_value)
plot(commB.fit.b3.4.none.ord$scaled.lvs[,2]~habitatN$weather_value)

# correlation matrix of 2 latent variables (1, 2) with all the Landsat channels. Look only at the two left columns
lvsmatrix <- habitatN %>% dplyr::select(starts_with("x"))
lvsmatrix <- cbind(commB.fit.b3.4.none.ord$scaled.lvs, lvsmatrix)
lvsmatrix_cor <- cor(lvsmatrix)
par(mfrow = c(1,1))
corrplot(lvsmatrix_cor, type = "lower", tl.pos = "l", tl.cex = 0.5, sig.level = 0.05, insig = "blank")

```

## mvabund
Testing whether i can combine levels within habitatN$habitat. I use mvabund to test whether MP and NF are significantly different from each other and from the other habitats:  BB, EC, JC, and CL.  I do this by creating two models, one with all 6 levels and one with NF|MP combined with one of the other two habitats. I run mvabund on both models and use anova to compare the two models. If significantly different, then the two habitat types are significantly different. Finally, i conservatively adjust the p-values for all 15 possible pairwise comparisons (6 * 5)/2. 

*anova.manyglm* options
    *test = "score"* # anova.manyglm help file says that test="wald" is poor for binomial data under some conditions. "score" is the better alternative. 
    *cor.type = "shrink"* # estimates correlations between species, but in an efficient way, which is necessary for our kind of dataset, where there are many more species than there are samples


```{r test if NF is sig diff from other habitats}
# communityB <- communityB[ , 1:150]  # comment away if i want to use the whole dataset.  this is to produce a partial dataset for debugging
communityB.mvb <- mvabund(communityB) 

#nboot <- 999 # set to 50 for debugging (~25 mins for nboot = 499)
nboot <- 50 # set to 999 for publication


# base model with no levels combined
mod_BBCLECJCMPNF.mvb <- manyglm(communityB.mvb ~ Habitat, data = habitatN, family = binomial("cloglog"))
plot(mod_BBCLECJCMPNF.mvb)
anova(mod_BBCLECJCMPNF.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 5, score = 534.8, nBoot = 999, 20 mins.  The Habitat predictor has a sig effect

# combine NF and MP
habitatN$HabitatMPNF <- fct_collapse(habitatN$Habitat, MPNF = c("MP", "NF"))
fct_count(habitatN$HabitatMPNF)

mod_CLBBECJC_MPNF.mvb <- manyglm(communityB.mvb ~ HabitatMPNF, data = habitatN, family = binomial("cloglog"))
plot(mod_CLBBECJC_MPNF.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLBBECJC_MPNF.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot) # p = 0.001, df = 1, score= 55.42, nBoot = 999.  MP and NF are sig diff

# combine NF and BB
habitatN$HabitatNFBB <- fct_collapse(habitatN$Habitat, NFBB = c("NF", "BB"))
fct_count(habitatN$HabitatNFBB)

mod_CLECJCMP_NFBB.mvb <- manyglm(communityB.mvb ~ HabitatNFBB, data = habitatN, family = binomial("cloglog"))
plot(mod_CLECJCMP_NFBB.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLECJCMP_NFBB.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 54.88, nBoot = 999, 20 mins.  BB and NF are sig diff

# combine NF and JC
habitatN$HabitatNFJC <- fct_collapse(habitatN$Habitat, NFJC = c("NF", "JC"))
fct_count(habitatN$HabitatNFJC)

mod_CLECBBMP_NFJC.mvb <- manyglm(communityB.mvb ~ HabitatNFJC, data = habitatN, family = binomial("cloglog"))
plot(mod_CLECBBMP_NFJC.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLECBBMP_NFJC.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 58.09, nBoot = 999, 20 mins.  JC and NF are sig diff

# combine NF and EC
habitatN$HabitatNFEC <- fct_collapse(habitatN$Habitat, NFEC = c("NF", "EC"))
fct_count(habitatN$HabitatNFEC)

mod_CLBBJCMP_NFEC.mvb <- manyglm(communityB.mvb ~ HabitatNFEC, data = habitatN, family = binomial("cloglog"))
plot(mod_CLBBJCMP_NFEC.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLBBJCMP_NFEC.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.003, df = 1, score= 54.41, nBoot = 999, 20 mins.  EC and NF are sig diff


# combine NF and CL
habitatN$HabitatNFCL <- fct_collapse(habitatN$Habitat, NFCL = c("NF", "CL"))
fct_count(habitatN$HabitatNFCL)

mod_ECBBJCMP_NFCL.mvb <- manyglm(communityB.mvb ~ HabitatNFCL, data = habitatN, family = binomial("cloglog"))
plot(mod_ECBBJCMP_NFCL.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_ECBBJCMP_NFCL.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 64.25, nBoot = 999, 20 mins.  CL and NF are sig diff


## do a table-wide correction, under the assumption that there are 15 possible pairwise comparisons between all 6 forest types:  (5*6)/2 = 15  
pvalues <- c(0.001, .001, .001, .003, .001)
pvalues.corr.fdr<-p.adjust(pvalues, method = "fdr", n = length(pvalues)) 
pvalues.corr.fdr
# 0.00125 0.00125 0.00125 0.00300 0.00125

```

```{r test if MP is sig diff from other habitats}
# communityB <- communityB[ , 1:150]  # comment away if i want to use the whole dataset.  this is to produce a partial dataset for debugging
communityB.mvb <- mvabund(communityB) 

# nboot <- 499 # set to 20 for debugging (~ 20 mins for 499)
nboot <- 50 # set to 999 for publication

# base model with no levels combined:  mod_BBCLECJCMPNF.mvb

# combine NF and MP:  already done above

# combine MP and BB
habitatN$HabitatMPBB <- fct_collapse(habitatN$Habitat, MPBB = c("MP", "BB"))
fct_count(habitatN$HabitatMPBB)

mod_CLECJCNF_MPBB.mvb <- manyglm(communityB.mvb ~ HabitatMPBB, data = habitatN, family = binomial("cloglog"))
plot(mod_CLECJCNF_MPBB.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLECJCNF_MPBB.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 54.23, nBoot = 999, 20 mins.  BB and MP are sig diff


# combine MP and JC
habitatN$HabitatMPJC <- fct_collapse(habitatN$Habitat, MPJC = c("MP", "JC"))
fct_count(habitatN$HabitatMPJC)

mod_CLECBBNF_MPJC.mvb <- manyglm(communityB.mvb ~ HabitatMPJC, data = habitatN, family = binomial("cloglog"))
plot(mod_CLECBBNF_MPJC.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLECBBNF_MPJC.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 58.73, nBoot = 999, 20 mins.  JC and MP are sig diff


# combine MP and EC
habitatN$HabitatMPEC <- fct_collapse(habitatN$Habitat, MPEC = c("MP", "EC"))
fct_count(habitatN$HabitatMPEC)

mod_CLBBJCNF_MPEC.mvb <- manyglm(communityB.mvb ~ HabitatMPEC, data = habitatN, family = binomial("cloglog"))
plot(mod_CLBBJCNF_MPEC.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLBBJCNF_MPEC.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 55.29, nBoot = 999, 20 mins.  EC and MP are sig diff


# combine MP and CL
habitatN$HabitatMPCL <- fct_collapse(habitatN$Habitat, MPCL = c("MP", "CL"))
fct_count(habitatN$HabitatMPCL)

mod_ECBBJCNF_MPCL.mvb <- manyglm(communityB.mvb ~ HabitatMPCL, data = habitatN, family = binomial("cloglog"))
plot(mod_ECBBJCNF_MPCL.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_ECBBJCNF_MPCL.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 71.41, nBoot = 999, 20 mins.  CL and MP are sig diff


# do a table-wide correction, under the assumption that there are 30 possible pairwise comparisons between all 6 forest types:  (5*6)/2 = 15
pvalues <- c(0.001, .001, .001, .001)
pvalues.corr.fdr<-p.adjust(pvalues, method = "fdr", n = length(pvalues)) 
pvalues.corr.fdr 
# [1] 0.001 0.001 0.001 0.001

# pvalues.corr.fdr<-p.adjust(pvalues, method = "fdr", n = length(pvalues))
```

# we also need a new mvabund analysis:  elevation + habitat + elevation*habitat
# if the interaction is not sig, then reduce to elevation + habitat and run again
```{r mvabud test for ele_hab}

communityB.mvb <- mvabund(communityB) 
#nboot <- 499 # set to 20 for debugging (~25 mins for nboot = 499)
nboot <- 50 # set to 999 for publication

# base model with Elevation_m+Habitat+Elevation_m*Habitat
mod_ele_hab_x.mvb <- manyglm(communityB.mvb ~ Elevation_m+Habitat+Elevation_m*Habitat, data = habitatN, family = binomial("cloglog"))

plot(mod_ele_hab_x.mvb)
anova(mod_ele_hab_x.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  
# Model: manyglm(formula = communityB.mvb ~ Elevation_m + Habitat + Elevation_m * 
# Model:     Habitat, family = binomial("cloglog"), data = habitatN)
# 
# Multivariate test:
#                     Res.Df Df.diff score Pr(>score)    
# (Intercept)             60                             
# Elevation_m             59       1 195.8      0.001 ***
# Habitat                 54       5 829.0      0.001 ***
# Elevation_m:Habitat     49       5 230.0      0.001 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

#mod_ele_hab.mvb <- manyglm(communityB.mvb ~ Elevation_m+Habitat, data = habitatN, family = binomial("cloglog"))

#plot(mod_ele_hab.mvb)
#anova(mod_ele_hab.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  
# Model: manyglm(formula = communityB.mvb ~ Elevation_m + Habitat, family = binomial("cloglog"), 
# Model:     data = habitatN)
# 
# Multivariate test:
#             Res.Df Df.diff score Pr(>score)    
# (Intercept)     60                             
# Elevation_m     59       1 116.4      0.001 ***
# Habitat         54       5 557.4      0.001 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

```



