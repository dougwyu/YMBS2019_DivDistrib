---
title: "Sichuan2014_metabarcode pipeline - simplified"
author: "Douglas Yu and Xiaoyang Wang"
date: "29/09/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Install packages
```{r eval=FALSE, include=FALSE}
install.packages(c("ade4","ape","beanplot","beepr","betapart","BiocManager","boral","car","conflicted","corrplot","data.table","iNEXT","iNextPD","metacoder","mvabund","phyloseq","RColorBrewer","renv","rmarkdown","SpeciesMix","tidyverse","UpSetR","vegan"), dependencies = TRUE)

library(BiocManager)
BiocManager::install(c("GenomicRanges", "Biobase", "IRanges", "AnnotationDbi", "phyloseq", "GenomicAlignments", "Organism.dplyr")) # install Bioconductor packages

library(devtools)
install_github("tobiasgf/lulu")
```

# start R analysis
```{r setup, packages}
library(ape) #read.tree() 
library(tidyverse)
library(vegan)
library(beanplot)
library(car)
library(iNEXT)
library(iNextPD)
library(ade4)
library(boral)
library(mvabund)
library(RColorBrewer)
library(betapart)
library(SpeciesMix)
library(beepr)
library(corrplot)
library(lulu)
library(phyloseq)
library(data.table)
library(metacoder)
library(UpSetR)
library(conflicted)
    conflict_prefer("filter", "dplyr")
    conflict_prefer("select", "dplyr")
    # [conflicted] Will prefer dplyr::filter over any other package
sessionInfo()
```

#After CROP clustering with 97% similarity
#run following command on terminal for getting match_list
vsearch --usearch_global 2libs_CROP97.cluster3507.fasta --db 2libs_CROP97.cluster3507.fasta --self --id .84 --iddef 1 --userout match_list.txt -userfields query+target+id --maxaccepts 0 --query_cov .9 --maxhits 10

# lulu OTU collapsing
```{r code for lulu, eval=FALSE, include=FALSE}
comLuLu <- read.table("lulu_3507/2libs_CROP97_merge_otu3507.txt", header= T, sep = "\t") # OTU table, 3507 OTUs
View(comLuLu)
comLuLu <- column_to_rownames(comLuLu, var = "Cluster")

matchlist <- read.table("lulu_3507/match_list.txt", header=FALSE,as.is=TRUE, stringsAsFactors=FALSE)  # use vsearch or usearch or blast to compare every OTU with every OTU and record the OTU pairs that have "high" similarity. Look at column v3 (similarity)

curated_result <- lulu(comLuLu, matchlist) # default parameters:
# curated_result <- lulu(comLuLu, matchlist, minimum_ratio_type = "min", minimum_ratio = 1, minimum_match = 84, minimum_relative_cooccurence = 0.95)

new_table_lulu <- curated_result$curated_table
# 3507 collapsed to 1507 OTUs
```

#we do phyloseq after lulu
```{r phyloseq, eval=FALSE, include=FALSE}
########## filter out small OTUs, phyloseq ##
communityAll_t <- new_table_lulu

communityAll <- t(communityAll_t)
TotalCounts <- c(colSums(communityAll))

tdt = data.table(colnames(communityAll),TotalCounts = colSums(communityAll),OTU = colnames(communityAll))

ggplot(tdt, aes(TotalCounts)) + 
  geom_histogram() + 
  ggtitle("Histogram of Total Counts")

tdt[(TotalCounts <= 0), .N]
tdt[(TotalCounts <= 1), .N]
tdt[(TotalCounts <= 2), .N]

taxcumsum = tdt[, .N, by = TotalCounts]
setkey(taxcumsum, TotalCounts)
taxcumsum[, CumSum := cumsum(N)]
# Define the plot
pCumSum = ggplot(taxcumsum, aes(TotalCounts, CumSum)) + 
  geom_point() +
  xlab("Filtering Threshold, Minimum Total Counts") +
  ylab("OTUs Filtered") +
  ggtitle("OTUs that would be filtered vs. the minimum count threshold")
pCumSum
pCumSum + xlim(0, 40)
pCumSum + xlim(0, 50)
pCumSum + xlim(0, 60)
pCumSum + xlim(0, 80)
pCumSum + xlim(0, 100)
pCumSum + xlim(0, 200)
pCumSum + xlim(0, 300)
pCumSum + xlim(0, 400)

## phyloseq-class experiment-level object
## otu_table()   OTU Table:         [ 3426 taxa and 34 samples ]
## sample_data() Sample Data:       [ 34 samples by 11 sample variables ]
## tax_table()   Taxonomy Table:    [ 3426 taxa by 7 taxonomic ranks ]
## phy_tree()    Phylogenetic Tree: [ 3426 tips and 3424 internal nodes ]

# after viewing the above figures, we choose 44 as the minimum OTU size

commMBC.44 <- communityAll[ ,colSums(communityAll)>=44] # 594 otus remain
commMBC.44 <- commMBC.44[ rowSums(commMBC.44)>0,]
rowSums(commMBC.44)
commMBC44copies <- t(commMBC.44)

write.table(commMBC44copies, file = "lulu_3507/2014MBC_44reads_otu594.txt", sep = "\t", row.names = TRUE, col.names = TRUE)
```

## commnunity analyses start from here
```{r load and format data for community analyses}

inputfile <- "./data/2014MBC_44reads_otu543_LandsatEnv.txt" # post phyloseq filtering and filtering for trees, using final bioinformatics pipeline with vsearch and RDP Classifier and phyloseq at min20reads, with landsat data

# command from readr package, with options on formatting the columns
gfgMB <- read_tsv(
   inputfile, col_names = TRUE, na = "NA",
   col_types = cols(
     Site = col_character(),
     Habitat = col_factor(c("BB", "CL", "EC", "JC", "MP", "NF")),
     Type = col_factor(c("1", "2", "3", "4", "5", "6")),
     Altitude = col_integer(),
     sampling_time = col_date(format = "%d/%m/%Y"),
     weather_value = col_factor(c("cloudy", "sunny", "rainy")),
     Landsat_value = col_factor(c("1", "2", "3", "4", "5", "6")),
     longitude = col_double(),
     latitude = col_double(),
     Elevation_m = col_integer()
     )
 )

gfgMB <- tbl_df(gfgMB)

# with(gfgMB, plot(Altitude ~ Elevation_m, col=as.numeric(Habitat)))  # Altitude variable is not reliable;  use Elevation_m instead, which is calculated from DEM
```

```{r environment dataset}
# make environment dataset
habitat <- gfgMB %>% dplyr::select(Site:Simpson_evenness_index)
habitat <- habitat %>% dplyr::filter(!is.na(Habitat)) # remove taxonomy rows
colnames(habitat)[11:80] <- paste0("x", colnames(habitat)[11:80]) # column names need to start with a letter
```

```{r community build}
community <- gfgMB %>% dplyr::select(starts_with("Cluster"))
# otuvector <- colnames(community)
community_t <- t(community)
community_t <- as.data.frame(community_t)
community_t <- rownames_to_column(community_t)
colnames(community_t) <- c("otu", gfgMB$Site) # add column names

communityAll <- t(community_t)
colvector <- communityAll[1,] # make a vector of the first row, which has the otu names
communityAll <- as.data.frame(communityAll)
colnames(communityAll) <-  colvector # add the otu names to the column names
communityAll <- communityAll[-1,] # remove first row, which has the column names
# convert the columns to numeric from factor
# http://stackoverflow.com/questions/2288485/how-to-convert-a-data-frame-column-to-numeric-type
communityAll <- sapply(communityAll, function(x) as.numeric(as.character(x))) # sapply applies a function to each column, and the function is:  function(x) as.numeric(as.character(x)).  Cannot convert factors to numeric directly. first convert to character, then to numeric
communityAll <- as.data.frame(communityAll) # then convert to df
```

# visualise a histogram of read number per OTU
```{r histogram of otu read numbers}
##### calculate distribution of read numbers per OTU to set minimum number 
otureads <- c(colSums(communityAll)) # list of the reads per OTU
sum(otureads) ## 1,900,539 reads total 
otureads[otureads>5000] <- 5000 # to make the histogram readable
otuhist <- hist(otureads, breaks = 100)
text(otuhist$mids, otuhist$counts, cex = 0.5, otuhist$counts, adj = c(.5, -.5), col = "blue3")
```

# Starting ecological analysis
Filter out sites with (1) low reads (<= 100), (2) very low numbers of species
```{r filter out sites }
community <- communityAll
#community <- t(new_table_lulu)
community <- as.data.frame(community)
#sort(colSums(community))
community[community < 5] <- 0 # set small cells to 0.

habitat$rowsums <- rowSums(community)
habitat$sprichness <- specnumber(community, MARGIN = 1) # number of species per site
# keep only sites that have more than 100 reads (removed 2, still remain 68)
community <- community %>% dplyr::filter(habitat$rowsums > 100)
habitatN <- habitat %>% dplyr::filter(habitat$rowsums > 100)
rowSums(community)

# keep only sites that have >=5 species (removed 68 - 61 = 7 sites)
community <- community %>% dplyr::filter(habitatN$sprichness >= 5)
habitatN <- habitatN %>% dplyr::filter(habitatN$sprichness >= 5)
#sort(colSums(community))
habitatN <- droplevels(habitatN)
community <- community[, colSums(community)>=20] # 7 OTUs removed
#Cluster31064  Cluster334816  Cluster672922  Cluster932857  Cluster989447  Cluster273565 Cluster1265861
```


```{r beanplot of read number by habitat}
beanplot(rowSums(community)~habitatN$Habitat, col = c("grey", "white"), xlab = "Habitat type", ylab = "Reads number")
# Kampstra, P. Beanplot: A Boxplot Alternative for Visual Comparison of Distributions. Journal of Statistical Software, Code Snippets 28(1). 1-9 (2008) 

nboot <- 999 # set to 999 for publication
# base model with no levels combined
reads.glm <- manyglm(rowSums(community) ~ habitatN$Habitat)
plot(reads.glm)
anova(reads.glm, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.036, df = 5, score = 8.671, nBoot = 999, 1 min. all read numbers are not significantly defferent from each other

```

# we used the taxonomic assignment (RDP classifier) results to built a taxon distribution of all OTUs
```{r metacoder, warning=FALSE}
#### for otus taxa tree

mbc_otus <- read.table("data/forMetacoder/2014MBC_otu536_tax.txt", header = T, sep = "\t")
mbc_samples <- read.table("data/forMetacoder/2014MBC_sample536.txt", header = T, sep = "\t")

mbc_otus[mbc_otus>1] <- 1 # using presence/absance data

#str(mbc_otus)
#str(mbc_samples)
## change data type 
mbc_otus$OTU_id <- as.character(mbc_otus$OTU_id)
mbc_otus$lineage <- as.character(mbc_otus$lineage)
mbc_samples$Site <- as.character(mbc_samples$Site)
mbc_samples$Habitat <- as.character(mbc_samples$Habitat)
##
mbc_otus <- as_tibble(mbc_otus)
mbc_samples <- as_tibble(mbc_samples)
#print(mbc_otus)
#print(mbc_samples)

obj <- parse_tax_data(mbc_otus, class_cols = "lineage", class_sep = ";",
                      class_key = c(tax_rank = "info", tax_name = "taxon_name"),
                      class_regex = "^(.+)__(.+)$")

# This returns a taxmap object. The taxmap class is designed to store any number of tables, lists, or vectors associated with taxonomic information and facilitate manipulating the data in a cohesive way. Here is what that object looks like:

print(obj) # or click on the object in the Environment pane

# accounting for un-even sampling
obj$data$tax_data <- calc_obs_props(obj, "tax_data")

print(obj)

# Getting per-taxon information
# Currently, we have values for the abundance of each OTU, not each taxon. To get information on the taxa, we can sum the abundance per-taxon like so:
obj$data$tax_abund <- calc_taxon_abund(obj, "tax_data",
                                       cols = mbc_samples$Site)
print(obj)
# Note that there is now an additional table with one row per taxon.
# We can also easily calculate the number of samples have reads for each taxon:
obj$data$tax_occ <- calc_n_samples(obj, "tax_abund", groups = mbc_samples$Habitat)
print(obj)

# Plotting taxonomic data
# Now that we have per-taxon information, we can plot the information using heat trees. The code below plots the number of “Nose” samples that have reads for each taxon. It also plots the number of OTUs assigned to each taxon in the overall dataset.

heat_tree(obj, 
          node_label = obj$taxon_names(),
          node_size = obj$n_obs(),
          node_color = obj$data$tax_occ$NF, 
          node_size_axis_label = "OTU count",
          node_color_axis_label = "Samples with reads")

# Comparing any number of treatments/groups
obj$data$diff_table <- compare_groups(obj, dataset = "tax_abund",
                                      cols = mbc_samples$Site,
                                      groups = mbc_samples$Habitat)
print(obj$data$diff_table)
heat_tree_matrix(obj,
                 data = "diff_table",
                 node_size = n_obs,
                 node_label = taxon_names,
                 node_color = log2_median_ratio,
                 node_color_range = diverging_palette(),
                 node_color_trans = "linear",
                 node_color_interval = c(-3, 3),
                 edge_color_interval = c(-3, 3),
                 node_size_axis_label = "Number of OTUs",
                 node_color_axis_label = "Log2 ratio median proportions")

# page(compare_groups) #check codes of compare_groups, change "median" to "mean"

##### remove CL group to see differences in forest and plantations only 
otus_without_CL <- mbc_otus %>% dplyr::select(-c(CL01:CL16))
samples_without_CL <- mbc_samples %>% dplyr::filter(mbc_samples$Habitat %in% c("BB", "EC", "JC", "MP", "NF"))

print(otus_without_CL)
print(samples_without_CL)
#str(otus_without_CL)
#str(samples_without_CL)
objF <- parse_tax_data(otus_without_CL, class_cols = "lineage", class_sep = ";",
                      class_key = c(tax_rank = "info", tax_name = "taxon_name"),
                      class_regex = "^(.+)__(.+)$")

# This returns a taxmap object. The taxmap class is designed to store any number of tables, lists, or vectors associated with taxonomic information and facilitate manipulating the data in a cohesive way. Here is what that object looks like:

print(objF) # or click on the object in the Environment pane

# removing low abundance counts
#objF$data$tax_data <- zero_low_counts(objF, "tax_data", min_count = 5)
#no_reads <- rowSums(objF$data$tax_data[, samples_without_CL$Site]) == 0
#sum(no_reads)
#objF <- filter_obs(objF, "tax_data", ! no_reads, drop_taxa = TRUE)
#print(objF)

# accounting for un-even sampling
objF$data$tax_data <- calc_obs_props(objF, "tax_data")

print(objF)

# Getting per-taxon information
# Currently, we have values for the abundance of each OTU, not each taxon. To get information on the taxa, we can sum the abundance per-taxon like so:
objF$data$tax_abund <- calc_taxon_abund(objF, "tax_data",
                                       cols = samples_without_CL$Site)
print(objF)
# Note that there is now an additional table with one row per taxon.
# We can also easily calculate the number of samples have reads for each taxon:
objF$data$tax_occ <- calc_n_samples(objF, "tax_abund", groups = samples_without_CL$Habitat)
print(objF)

# Plotting taxonomic data
# Now that we have per-taxon information, we can plot the information using heat trees. The code below plots the number of “Nose” samples that have reads for each taxon. It also plots the number of OTUs assigned to each taxon in the overall dataset.

heat_tree(objF, 
          node_label = objF$taxon_names(),
          node_size = objF$n_obs(),
          node_color = objF$data$tax_occ$NF, 
          node_size_axis_label = "OTU count",
          node_color_axis_label = "Log2 ratio median proportions")

# Comparing any number of treatments/groups
objF$data$diff_table <- compare_groups(objF, dataset = "tax_abund",
                                      cols = samples_without_CL$Site,
                                      groups = samples_without_CL$Habitat)
print(objF$data$diff_table)
heat_tree_matrix(objF,
                 data = "diff_table",
                 node_size = n_obs,
                 node_label = taxon_names,
                 node_color = log2_median_ratio,
                 node_color_range = diverging_palette(),
                 node_color_trans = "linear",
                 node_color_interval = c(-3, 3),
                 edge_color_interval = c(-3, 3),
                 node_size_axis_label = "Number of OTUs",
                 node_color_axis_label = "Log2 ratio median proportions")


```

# Alpha Diversity
```{r make presence/absence dataset}
communityB <- community
communityB[communityB>1] <- 1 # binary
rownames(communityB) # 61 rows = sites
```

```{r beanplot}
#beanplot(specnumber(communityB)~habitatN$Habitat, col = c("grey", "white"))

beanplot(specnumber(communityB)~habitatN$Habitat, col = c("grey", "white"), ylab = "Species richness")
```

```{r each land-cover type community data}
BB <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("BB"))
CL <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("CL"))
EC <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("EC"))
JC <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("JC"))
MP <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("MP"))
NF <- communityB %>% dplyr::filter(habitatN$Habitat %in% c("NF"))

```

Test 2
```{r t-test for observed species richness}

BB_obs <- rowSums(BB)
CL_obs <- rowSums(CL)
EC_obs <- rowSums(EC)
JC_obs <- rowSums(JC)
MP_obs <- rowSums(MP)
NF_obs <- rowSums(NF)

vec <- rep(NA, 15)
vec[1] <- t.test(BB_obs, CL_obs)[["p.value"]]
vec[2] <- t.test(BB_obs, EC_obs)[["p.value"]]
vec[3] <- t.test(BB_obs, JC_obs)[["p.value"]]
vec[4] <- t.test(BB_obs, MP_obs)[["p.value"]]
vec[5] <- t.test(BB_obs, NF_obs)[["p.value"]]
vec[6] <- t.test(CL_obs, EC_obs)[["p.value"]]
vec[7] <- t.test(CL_obs, JC_obs)[["p.value"]]
vec[8] <- t.test(CL_obs, MP_obs)[["p.value"]]
vec[9] <- t.test(CL_obs, NF_obs)[["p.value"]]
vec[10] <- t.test(EC_obs, JC_obs)[["p.value"]]
vec[11] <- t.test(EC_obs, MP_obs)[["p.value"]]
vec[12] <- t.test(EC_obs, NF_obs)[["p.value"]]
vec[13] <- t.test(JC_obs, MP_obs)[["p.value"]]
vec[14] <- t.test(JC_obs, NF_obs)[["p.value"]]
vec[15] <- t.test(MP_obs, NF_obs)[["p.value"]]

p_values <- vec
# p_values <- c(0.0244, 0.2402, 0.1078, 0.7774, 0.2095, 0.0027, 0.0004, 0.0142, 0.6047, 0.7341, 0.3663, 0.0429, 0.1877, 0.0202, 0.1514)
p_values.corr.fdr<-p.adjust(p_values, method = "fdr", n = length(p_values)) 
p_values.corr.fdr
# 0.073117886 0.327556823 0.231087068 0.777408689 0.314280549 0.020438641 0.005402277 0.070797333
# 0.697709778 0.777408689 0.457845891 0.107319818 0.312783984 0.073117886 0.283870359
```

# traditional Chao
```{r specpool, }
######## otu table with original reads number
(pool1 <- specpool(communityB, habitatN$Habitat))

```

   Species     chao  chao.se     jack1 jack1.se    jack2     boot   boot.se  n
BB      83 185.9796 39.89775 132.71429 20.90210 165.8095 104.1117  8.989364  7
CL     194 336.1658 37.44452 295.73333 32.32777 358.8143 238.0445 15.679575 15
EC      64 115.4592 22.33722  99.14286 16.06619 120.0952  79.3403  7.646678  7
JC      85 209.6154 48.52236 139.00000 23.08246 177.7556 107.5053 10.816630 10
MP     119 405.5079 98.01090 203.44444 33.92075 267.8056 153.4993 14.358124  9
NF     210 507.3394 72.71267 346.61538 48.44615 445.4744 266.7256 22.180423 13

```{r barchart for estimating value}
es_value <- pool1$chao
se <- pool1$chao.se
# es_value <- c(185.9796, 115.4592, 209.6154, 405.5079, 336.1658, 507.3394)
# se <- c(39.89775, 22.33722, 48.52236, 98.01090, 37.44452, 72.71267)

error.bar <- function(x, y, upper, lower=upper, length=0.1,...){
if(length(x) != length(y) | length(y) !=length(lower) | length(lower) != length(upper))
stop("vectors must be same length")
arrows(x,y+upper, x, y-lower, angle=90, code=3, length=length, ...)
}

par(mfrow=c(1,2))
bar_es <- barplot(es_value, names.arg = c("BB", "EC", "JC", "MP", "CL", "NF"), col = "lightblue", ylim = c(0, 600), border = NA, ylab = "Species richness estimates")
error.bar(bar_es,es_value, se)
par(mfrow=c(1,1))
```

```{r Welch-t test for Chao}
############ This function (t.test2) will calculate Welch's test
t.test2 <- function(m1, m2, s1, s2, n1, n2, m0=0, equal.variance=FALSE)
{
  if( equal.variance==FALSE ) 
  {
    se <- sqrt( (s1^2/n1) + (s2^2/n2) )
    # welch-satterthwaite df
    df <- ( (s1^2/n1 + s2^2/n2)^2 )/( (s1^2/n1)^2/(n1-1) + (s2^2/n2)^2/(n2-1) )
  } else
  {
    # pooled standard deviation, scaled by the sample sizes
    se <- sqrt( (1/n1 + 1/n2) * ((n1-1)*s1^2 + (n2-1)*s2^2)/(n1+n2-2) ) 
    df <- n1+n2-2
  }      
  t <- (m1-m2-m0)/se 
  dat <- c(m1-m2, se, t, round(df,1), 2*pt(-abs(t),df))    
  # names(dat) <- c("Difference of means", "Std Error", "t", "df", "p-value")
  return(dat) 
}

vec <- rep(NA, 15)

# t test for BB and CL
vec[1] <- t.test2(pool1[1, 2], pool1[2, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[2, 3]*sqrt(pool1[2, 9]), pool1[1, 9], pool1[2, 9])[5]

# t test for BB and EC
vec[2] <- t.test2(pool1[1, 2], pool1[3, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[3, 3]*sqrt(pool1[3, 9]), pool1[1, 9], pool1[3, 9])[5]

# t test for BB and JC
vec[3] <- t.test2(pool1[1, 2], pool1[4, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[4, 3]*sqrt(pool1[4, 9]), pool1[1, 9], pool1[4, 9])[5]

# t test for BB and MP
vec[4] <- t.test2(pool1[1, 2], pool1[5, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[5, 3]*sqrt(pool1[5, 9]), pool1[1, 9], pool1[5, 9])[5]

# t test for BB and NF
vec[5] <- t.test2(pool1[1, 2], pool1[6, 2], pool1[1, 3]*sqrt(pool1[1, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[1, 9], pool1[6, 9])[5]

# t test for CL and EC
vec[6] <- t.test2(pool1[2, 2], pool1[3, 2], pool1[2, 3]*sqrt(pool1[2, 9]), pool1[3, 3]*sqrt(pool1[3, 9]), pool1[2, 9], pool1[3, 9])[5]

# t test for CL and JC
vec[7] <- t.test2(pool1[2, 2], pool1[4, 2], pool1[2, 3]*sqrt(pool1[2, 9]), pool1[4, 3]*sqrt(pool1[4, 9]), pool1[2, 9], pool1[4, 9])[5]

# t test for CL and MP
vec[8] <- t.test2(pool1[2, 2], pool1[5, 2], pool1[2, 3]*sqrt(pool1[2, 9]), pool1[5, 3]*sqrt(pool1[5, 9]), pool1[2, 9], pool1[5, 9])[5]

# t test for CL and NF
vec[9] <- t.test2(pool1[2, 2], pool1[6, 2], pool1[2, 3]*sqrt(pool1[2, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[2, 9], pool1[6, 9])[5]

# t test for EC and JC
vec[10] <- t.test2(pool1[3, 2], pool1[4, 2], pool1[3, 3]*sqrt(pool1[3, 9]), pool1[4, 3]*sqrt(pool1[4, 9]), pool1[3, 9], pool1[4, 9])[5]

# t test for EC and MP
vec[11] <- t.test2(pool1[3, 2], pool1[5, 2], pool1[3, 3]*sqrt(pool1[3, 9]), pool1[5, 3]*sqrt(pool1[5, 9]), pool1[3, 9], pool1[5, 9])[5]

# t test for EC and NF
vec[12] <- t.test2(pool1[3, 2], pool1[6, 2], pool1[3, 3]*sqrt(pool1[3, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[3, 9], pool1[6, 9])[5]

# t test for JC and MP
vec[13] <- t.test2(pool1[4, 2], pool1[5, 2], pool1[4, 3]*sqrt(pool1[4, 9]), pool1[5, 3]*sqrt(pool1[5, 9]), pool1[4, 9], pool1[5, 9])[5]

# t test for JC and NF
vec[14] <- t.test2(pool1[4, 2], pool1[6, 2], pool1[4, 3]*sqrt(pool1[4, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[4, 9], pool1[6, 9])[5]

# t test for MP and NF
vec[15] <- t.test2(pool1[5, 2], pool1[6, 2], pool1[5, 3]*sqrt(pool1[5, 9]), pool1[6, 3]*sqrt(pool1[6, 9]), pool1[5, 9], pool1[6, 9])[5]

# BB_CL, BB_EC, BB_JC, BB_MP, BB_NF, CL_EC, CL_JC, CL_MP, CL_NF, EC_JC, EC_MP, EC_NF, JC_MP, JC_NF, MP_NF
p_values <- vec
p_values.corr.fdr<-p.adjust(p_values, method = "fdr", n = length(p_values)) 
p_values.corr.fdr
#  [1] 0.0432930174 0.1948572203 0.7119988942 0.1058460773 0.0059727485 0.0009117642 0.0996143684
#  [8] 0.5604343529 0.0996143684 0.1399044218 0.0459757611 0.0010730017 0.1399044218 0.0106130314
# [15] 0.4803785786

```

```{r make community abun data}
cname <- c("BB","CL","EC","JC","MP","NF")

comm4inext_abun <- matrix(c(colSums(BB), colSums(CL), colSums(EC), colSums(JC), colSums(MP), colSums(NF)), ncol = 6) # BB, CL, EC, JC, MP, NF from 'r t-test for observed species richness'

colnames(comm4inext_abun) <- cname
colnameBB <- colnames(BB)
rownames(comm4inext_abun) <- colnameBB

```

# Interpolation and extrapolation of Hill number 
```{r iNEXT}
# http://chao.stat.nthu.edu.tw/wordpress/wp-content/uploads/software/iNEXT_UserGuide.pdf

comm4inext <- rbind(c(nrow(BB), nrow(CL), nrow(EC), nrow(JC), nrow(MP), nrow(NF)), comm4inext_abun) #
#comm4inext

confnum=0.95 # set confidence here
outcomm0 <- iNEXT(comm4inext, q=0, conf=confnum, datatype="incidence_freq")
# Hill numbers (q):  0 = sp richness, 1 = Shannon, 2 = inverse Simpson
outcomm0$DataInfo
ChaoRichness(comm4inext, datatype="incidence_freq") # same as specpool results, so i trust that we have done this correctly
ChaoShannon(comm4inext, datatype="incidence_freq")

outI <- iNEXT(comm4inext, q=c(0,1,2), conf=confnum, datatype="incidence_freq")
# Sample‐size‐based R/E curves, separating by "site"
ggiNEXT(outI, type=1, facet.var="order")+theme_bw(base_size = 18)

```

```{r iNextPD, warning=FALSE }
commPD <- communityB
# commPD$Cluster418132 #
# commPD$Cluster636975 #
# commPD$Cluster549885 #
commPD$Cluster99619

#remove 3 OTUs with very long branches
commPD$Cluster418132 <- NULL
commPD$Cluster636975 <- NULL
commPD$Cluster549885 <- NULL

BB <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("BB"))
CL <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("CL"))
EC <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("EC"))
JC <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("JC"))
MP <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("MP"))
NF <- commPD %>% dplyr::filter(habitatN$Habitat %in% c("NF"))
comm4inextPD <- matrix(c(colSums(BB), colSums(CL), colSums(EC), colSums(JC), colSums(MP), colSums(NF)), ncol = 6)

colnames(comm4inextPD) <- cname
colnameBB <- colnames(BB)
rownames(comm4inextPD) <- colnameBB

MLtree.tre <- read.table("./data/2014MBC_535otu-2collembola_align533.newick")
ML.tre <- ade4::newick2phylog(MLtree.tre$V1)
ML.lab <- rownames(comm4inextPD)

#rownames(comm4inext_abun)
BBnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("BB"))
CLnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("CL"))
ECnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("EC"))
JCnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("JC"))
MPnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("MP"))
NFnames <- habitatN %>% dplyr::filter(habitatN$Habitat %in% c("NF"))

rownames(BB) <- BBnames$Site
rownames(CL) <- CLnames$Site
rownames(EC) <- ECnames$Site
rownames(JC) <- JCnames$Site
rownames(MP) <- MPnames$Site
rownames(NF) <- NFnames$Site

BBnames$Site

#commB <- list(BB.Site = t(BB), CL.Site = t(CL), EC.Site = t(EC), JC.Site = t(JC), MP.Site = t(MP), NF.Site = t(NF))
commB <- list(BB = t(BB), CL = t(CL), EC = t(EC), JC = t(JC), MP = t(MP), NF = t(NF))


out <- iNextPD(commB, ML.lab, ML.tre, q=c(0, 1, 2), datatype="incidence_raw", endpoint = 30, se = TRUE)
# Sample‐size‐based R/E curves, separating by "site""
#ggiNEXT(out, type=1, facet.var="site") +theme_bw(base_size = 18)
# Sample‐size‐based R/E curves, separating by "order"
ggiNEXT(out, type=1, facet.var="order")+theme_bw(base_size = 18)
# display black‐white theme
#ggiNEXT(out, type=1, facet.var="order", grey=TRUE)

table.phylog(comm4inextPD, ML.tre, csize=2, f.phylog=0.7)

```

# Beta diversity turnover versus nestedness
## Run tabasco before removing zerotons and singletons
```{r tabasco}
community.jmds <- metaMDS(communityB, distance = "jaccard", trymax = 40, binary=TRUE)
community.jmds <- metaMDS(communityB, distance = "jaccard", binary = TRUE, previous.best = community.jmds)  # doesn't converge well, with final stress > 0.20
stressplot(community.jmds)
tabasco(community, use = community.jmds, labCol = habitatN$Habitat, col = brewer.pal(3, "Oranges"))
```

```{r UpSetR}
# In our case, Rows are OTUs, Columns are land-cover types (BB, EC, JC, MP, NF, CL). So this is pretty easy to make. 0/1 dataset
comm4inext_abun <- as.data.frame(comm4inext_abun) # comm4inext_abun from iNEXT
comm4inext_abun[comm4inext_abun>1] <- 1
colSums(comm4inext_abun)
# comm4inext_abun$MF <- comm4inext_abun$MP
#sort(rowSums(comm4inext_abun))
# upset(comm4inext_abun, nsets = 6, nintersects = 70, cutoff = 7, keep.order = TRUE)
#upset(comm4inext_abun, sets = c("NF", "MP", "BB", "EC", "JC", "CL"), nintersects = 70, group.by = "sets", cutoff = 7, keep.order = TRUE)
upset(comm4inext_abun, sets = c("NF", "MP", "BB", "EC", "JC", "CL"), cutoff = 6, nintersects = NA, group.by = "sets", keep.order = TRUE)
upset(comm4inext_abun, sets = c("NF", "MP", "BB", "EC", "JC", "CL"), nintersects = NA, group.by = "sets", keep.order = TRUE)
# upset(comm4inext_abun, sets = c("NF", "MF", "BB", "EC", "JC", "CL"), cutoff = 6, nintersects = NA, group.by = "sets", keep.order = TRUE)
# upset(comm4inext_abun, sets = c("NF", "MF", "BB", "EC", "JC", "CL"), nintersects = NA, group.by = "sets", keep.order = TRUE)

```

## Betapart analysis before removing zerotons and singletons
```{r betapart, warning=FALSE}
communityBbetapart <- bind_cols(habitatN, communityB) 
communityBbetapart <- communityBbetapart %>% dplyr::select(-c(Habitat:sprichness))

JCNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("JC", "NF")) %>% column_to_rownames(var="Site")
BBNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("BB", "NF")) %>% column_to_rownames(var="Site")
CLNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("CL", "NF")) %>% column_to_rownames(var="Site")
ECNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("EC", "NF")) %>% column_to_rownames(var="Site")
MPNF <- communityBbetapart %>% dplyr::filter(habitatN$Habitat %in% c("MP", "NF")) %>% column_to_rownames(var="Site")

JCNF.multi.dist <- beta.multi(JCNF, index.family="jac")
BBNF.multi.dist <- beta.multi(BBNF, index.family="jac")
CLNF.multi.dist <- beta.multi(CLNF, index.family="jac")
ECNF.multi.dist <- beta.multi(ECNF, index.family="jac")
MPNF.multi.dist <- beta.multi(MPNF, index.family="jac")

multi.all <- list(JCNF = JCNF.multi.dist, BBNF = BBNF.multi.dist, CLNF = CLNF.multi.dist, ECNF =  ECNF.multi.dist, MPNF = MPNF.multi.dist)


ALL.dist <- communityBbetapart %>% column_to_rownames(var="Site") %>% beta.pair(index.family="jac")
ALL.dist.subset <- ALL.dist[["beta.jne"]]
ALL.dist.jne.jmds <- metaMDS(ALL.dist.subset)
ALL.dist.jne.jmds <- metaMDS(ALL.dist.subset, previous.best = ALL.dist.jne.jmds)
stressplot(ALL.dist.jne.jmds)
ALL.dist.subset <- ALL.dist[["beta.jtu"]]
ALL.dist.jtu.jmds <- metaMDS(ALL.dist.subset)
ALL.dist.jtu.jmds <- metaMDS(ALL.dist.subset, previous.best = ALL.dist.jne.jmds)
stressplot(ALL.dist.jtu.jmds)


# , ylim = c(-0.5, 0.5), xlim = c(-0.4, 0.4)
par(mfrow=c(2,2))
colvec <- brewer.pal(5, "Set1")
with(habitatN, ordisurf(community.jmds, sprichness, main="All beta diversity", cex=0.5, col = "white"),  ylim = c(-0.5, 0.5))
# plot(community.jmds, main = "All beta diversity", ylim = c(-0.4, 0.4))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("BB"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[1]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("CL"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("EC"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("JC"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("MP"))))
    with(habitatN, ordispider(community.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("NF"))))

plot(ALL.dist.jtu.jmds, main = "Turnover beta diversity only", ylim = c(-0.5, 0.5))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("BB"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[1]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("CL"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("EC"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("JC"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("MP"))))
    with(habitatN, ordispider(ALL.dist.jtu.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("NF"))))
    
plot(ALL.dist.jne.jmds, main = "Nestedness beta diversity only", ylim = c(-0.5, 0.5))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("BB"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[1]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("CL"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("EC"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col=as.character(colvec[3]), alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("JC"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("MP"))))
    with(habitatN, ordispider(ALL.dist.jne.jmds, Habitat, cex=.5, draw="polygon", col="darkblue", alpha=20, kind="se", conf=0.95, label=TRUE, show.groups=(c("NF"))))
    
par(mfrow=c(1,1))

```

# Total beta diversity NMDS is correlated with turnover-only beta diversity 
```{r}
protest(community.jmds, ALL.dist.jtu.jmds)
```

Call:
protest(X = community.jmds, Y = ALL.dist.jtu.jmds) 

Procrustes Sum of Squares (m12 squared):        0.07381 
Correlation in a symmetric Procrustes rotation: 0.9624 
Significance:  0.001 

Permutation: free
Number of permutations: 999

Total beta diversity NMDS is not correlated with nestedness-only beta diversity
```{r}
protest(community.jmds, ALL.dist.jne.jmds)
```

Call:
protest(X = community.jmds, Y = ALL.dist.jne.jmds) 

Procrustes Sum of Squares (m12 squared):        0.9937 
Correlation in a symmetric Procrustes rotation: 0.07915 
Significance:  0.909 

Permutation: free
Number of permutations: 999

# Beta diversity UNCONSTRAINED ordination
############ IMPORTANT #####################################
## For NMDS, mvabund, and boral analyses, remove zerotons and singletons

```{r remove zerotons and singletons from communityB}
communityB <- communityB[, which(specnumber(communityB, MARGIN=2) > 1)]
# 267 species remained

```

## NMDS ordination
```{r NMDS}
### do NMDS analysis to quickly see patterns ####
community.jmds <- metaMDS(communityB, distance = "jaccard", trymax = 40, binary=TRUE)
community.jmds <- metaMDS(communityB, distance = "jaccard", binary = TRUE, previous.best = community.jmds)  # doesn't converge well, with final stress > 0.20
stressplot(community.jmds)
```

## boral
Tested different error families (testing code is archived down below).  Based on residuals, decided on family = "binomial," which is good because the dataset is presence/absence. Using row.effect = "random" to get a composition-only analysis.   Now rerun with many more iterations

# This takes a long time to run, should run overnight
```{r boral high iterations}
# set up MCMC parameters
# mcmc.control <- list(n.burnin = 300, n.iteration = 1000, n.thin = 30, seed = 123) # for debugging

mcmc.control4 <- list(n.burnin = 10000, n.iteration = 40000, n.thin = 30) 

# Set up priors
# Francis Hui suggests trying different priors to stabilise sampling for sparse matrices (such as we have).  This isn't as important as getting the mcmc.control parameters right
#set.prior <- list(type = c("normal","normal","normal","uniform"), hypparams = c(10, 10, 10, 30), ssvs.index = ssvsindex)  
# type = c("normal","normal","normal","uniform"), hypparams = c(10, 10, 10, 30) # boral default
# type = c("cauchy","cauchy","cauchy","uniform"), hypparams = c(2.5^2, 2.5^2, 2.5^2, 30) # Gelman proposed this
# type = c("normal","normal","normal","uniform"), hypparams = c(1, 1, 1, 30) ## People who developed the STAN package proposed this as one possibility. 


# set up model
commB.fit.b3.4.none <- boral(communityB, family = "binomial", num.lv = 2, row.eff = "random", mcmc.control = mcmc.control4, save.model = TRUE); beep(sound = 1)

# save output in case i want to do other analyses on it later
saveRDS(commB.fit.b3.4.none, "boral_commB.fit.b3.4.RDS")
# commB.fit.b3.4.none <- readRDS("boral_commB.fit.b3.4.RDS") # to restore

summary(commB.fit.b3.4.none)
par(mfrow = c(2,2))
plot(commB.fit.b3.4.none) ## Plots used in residual analysis, 
par(mfrow = c(1,1))

commB.fit.b3.4.none.ord <- lvsplot(commB.fit.b3.4.none, biplot=FALSE, col = as.numeric(habitatN$Habitat), return.vals = TRUE)

res.cors <- get.residual.cor(commB.fit.b3.4.none); beep(1) # residual deviation
res.cors$trace
# 4419.197
# habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,1]

cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,1], method = "pearson")
# t = -3.9509, df = 59, p-value = 0.0002106
# alternative hypothesis: true correlation is not equal to 0
# 95 percent confidence interval: -0.6359671 -0.2323392
# sample estimates: cor -0.4573986
cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,1], method = "kendall")
cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,1], method = "spearman")
# S = 56743, p-value = 4.022e-05
# alternative hypothesis: true rho is not equal to 0
# sample estimates: rho -0.5003503 

cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,2], method = "pearson")
# t = -5.0463, df = 59, p-value = 4.603e-06
# alternative hypothesis: true correlation is not equal to 0
# 95 percent confidence interval: -0.7036107 -0.3449532
#sample estimates: cor -0.5490776 

cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,2], method = "kendall")
cor.test(habitatN$Elevation_m, y = commB.fit.b3.4.none$lv.iqr[,2], method = "spearman")
# S = 59369, p-value = 1.645e-06
# alternative hypothesis: true rho is not equal to 0
#sample estimates: rho -0.5697854 

```

Try to interpret the latent variables. Latent variables are clearly correlated with the forest types (habitatN$Habitat) but weakly at best correlated with Simpson_evenness_index, Elevation_m, and weather_value. 
*There does seem to be a quadratic relationship between lvs2 and Elevation_m*. *The correlation plot suggests that there are some informative Landsat channels.*  
```{r boral lvs correlations}
cbind(commB.fit.b3.4.none.ord$scaled.lvs,habitatN$Habitat)
par(mfrow = c(1,2))
plot(commB.fit.b3.4.none.ord$scaled.lvs[,1]~habitatN$Habitat)
plot(commB.fit.b3.4.none.ord$scaled.lvs[,2]~habitatN$Habitat)

plot(commB.fit.b3.4.none.ord$scaled.lvs[,1]~habitatN$Simpson_evenness_index)
plot(commB.fit.b3.4.none.ord$scaled.lvs[,2]~habitatN$Simpson_evenness_index)

plot(commB.fit.b3.4.none.ord$scaled.lvs[,1]~habitatN$Elevation_m)
plot(commB.fit.b3.4.none.ord$scaled.lvs[,2]~habitatN$Elevation_m)

plot(commB.fit.b3.4.none.ord$scaled.lvs[,1]~habitatN$weather_value)
plot(commB.fit.b3.4.none.ord$scaled.lvs[,2]~habitatN$weather_value)

# correlation matrix of 2 latent variables (1, 2) with all the Landsat channels. Look only at the two left columns
lvsmatrix <- habitatN %>% dplyr::select(starts_with("x"))
lvsmatrix <- cbind(commB.fit.b3.4.none.ord$scaled.lvs, lvsmatrix)
lvsmatrix_cor <- cor(lvsmatrix)
par(mfrow = c(1,1))
corrplot(lvsmatrix_cor, type = "lower", tl.pos = "l", tl.cex = 0.5, sig.level = 0.05, insig = "blank")

```

## mvabund
Testing whether i can combine levels within habitatN$habitat. I use mvabund to test whether MP and NF are significantly different from each other and from the other habitats:  BB, EC, JC, and CL.  I do this by creating two models, one with all 6 levels and one with NF|MP combined with one of the other two habitats. I run mvabund on both models and use anova to compare the two models. If significantly different, then the two habitat types are significantly different. Finally, i conservatively adjust the p-values for all 15 possible pairwise comparisons (6 * 5)/2. 

*anova.manyglm* options
    *test = "score"* # anova.manyglm help file says that test="wald" is poor for binomial data under some conditions. "score" is the better alternative. 
    *cor.type = "shrink"* # estimates correlations between species, but in an efficient way, which is necessary for our kind of dataset, where there are many more species than there are samples


```{r test if NF is sig diff from other habitats}
# communityB <- communityB[ , 1:150]  # comment away if i want to use the whole dataset.  this is to produce a partial dataset for debugging
communityB.mvb <- mvabund(communityB) 

#nboot <- 499 # set to 20 for debugging (~25 mins for nboot = 499)
nboot <- 999 # set to 999 for publication


# base model with no levels combined
mod_BBCLECJCMPNF.mvb <- manyglm(communityB.mvb ~ Habitat, data = habitatN, family = binomial("cloglog"))
plot(mod_BBCLECJCMPNF.mvb)
anova(mod_BBCLECJCMPNF.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 5, score = 534.8, nBoot = 999, 20 mins.  The Habitat predictor has a sig effect

# combine NF and MP
habitatN$HabitatMPNF <- fct_collapse(habitatN$Habitat, MPNF = c("MP", "NF"))
fct_count(habitatN$HabitatMPNF)

mod_CLBBECJC_MPNF.mvb <- manyglm(communityB.mvb ~ HabitatMPNF, data = habitatN, family = binomial("cloglog"))
plot(mod_CLBBECJC_MPNF.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLBBECJC_MPNF.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot) # p = 0.001, df = 1, score= 55.42, nBoot = 999.  MP and NF are sig diff

# combine NF and BB
habitatN$HabitatNFBB <- fct_collapse(habitatN$Habitat, NFBB = c("NF", "BB"))
fct_count(habitatN$HabitatNFBB)

mod_CLECJCMP_NFBB.mvb <- manyglm(communityB.mvb ~ HabitatNFBB, data = habitatN, family = binomial("cloglog"))
plot(mod_CLECJCMP_NFBB.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLECJCMP_NFBB.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 54.88, nBoot = 999, 20 mins.  BB and NF are sig diff

# combine NF and JC
habitatN$HabitatNFJC <- fct_collapse(habitatN$Habitat, NFJC = c("NF", "JC"))
fct_count(habitatN$HabitatNFJC)

mod_CLECBBMP_NFJC.mvb <- manyglm(communityB.mvb ~ HabitatNFJC, data = habitatN, family = binomial("cloglog"))
plot(mod_CLECBBMP_NFJC.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLECBBMP_NFJC.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 58.09, nBoot = 999, 20 mins.  JC and NF are sig diff

# combine NF and EC
habitatN$HabitatNFEC <- fct_collapse(habitatN$Habitat, NFEC = c("NF", "EC"))
fct_count(habitatN$HabitatNFEC)

mod_CLBBJCMP_NFEC.mvb <- manyglm(communityB.mvb ~ HabitatNFEC, data = habitatN, family = binomial("cloglog"))
plot(mod_CLBBJCMP_NFEC.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLBBJCMP_NFEC.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.003, df = 1, score= 54.41, nBoot = 999, 20 mins.  EC and NF are sig diff


# combine NF and CL
habitatN$HabitatNFCL <- fct_collapse(habitatN$Habitat, NFCL = c("NF", "CL"))
fct_count(habitatN$HabitatNFCL)

mod_ECBBJCMP_NFCL.mvb <- manyglm(communityB.mvb ~ HabitatNFCL, data = habitatN, family = binomial("cloglog"))
plot(mod_ECBBJCMP_NFCL.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_ECBBJCMP_NFCL.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 64.25, nBoot = 999, 20 mins.  CL and NF are sig diff


## do a table-wide correction, under the assumption that there are 15 possible pairwise comparisons between all 6 forest types:  (5*6)/2 = 15  
pvalues <- c(0.001, .001, .001, .003, .001)
pvalues.corr.fdr<-p.adjust(pvalues, method = "fdr", n = length(pvalues)) 
pvalues.corr.fdr
# 0.00125 0.00125 0.00125 0.00300 0.00125

```

```{r test if MP is sig diff from other habitats}
# communityB <- communityB[ , 1:150]  # comment away if i want to use the whole dataset.  this is to produce a partial dataset for debugging
communityB.mvb <- mvabund(communityB) 

# nboot <- 499 # set to 20 for debugging (~ 20 mins for 499)
nboot <- 999 # set to 999 for publication

# base model with no levels combined:  mod_BBCLECJCMPNF.mvb

# combine NF and MP:  already done above

# combine MP and BB
habitatN$HabitatMPBB <- fct_collapse(habitatN$Habitat, MPBB = c("MP", "BB"))
fct_count(habitatN$HabitatMPBB)

mod_CLECJCNF_MPBB.mvb <- manyglm(communityB.mvb ~ HabitatMPBB, data = habitatN, family = binomial("cloglog"))
plot(mod_CLECJCNF_MPBB.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLECJCNF_MPBB.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 54.23, nBoot = 999, 20 mins.  BB and MP are sig diff


# combine MP and JC
habitatN$HabitatMPJC <- fct_collapse(habitatN$Habitat, MPJC = c("MP", "JC"))
fct_count(habitatN$HabitatMPJC)

mod_CLECBBNF_MPJC.mvb <- manyglm(communityB.mvb ~ HabitatMPJC, data = habitatN, family = binomial("cloglog"))
plot(mod_CLECBBNF_MPJC.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLECBBNF_MPJC.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 58.73, nBoot = 999, 20 mins.  JC and MP are sig diff


# combine MP and EC
habitatN$HabitatMPEC <- fct_collapse(habitatN$Habitat, MPEC = c("MP", "EC"))
fct_count(habitatN$HabitatMPEC)

mod_CLBBJCNF_MPEC.mvb <- manyglm(communityB.mvb ~ HabitatMPEC, data = habitatN, family = binomial("cloglog"))
plot(mod_CLBBJCNF_MPEC.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_CLBBJCNF_MPEC.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 55.29, nBoot = 999, 20 mins.  EC and MP are sig diff


# combine MP and CL
habitatN$HabitatMPCL <- fct_collapse(habitatN$Habitat, MPCL = c("MP", "CL"))
fct_count(habitatN$HabitatMPCL)

mod_ECBBJCNF_MPCL.mvb <- manyglm(communityB.mvb ~ HabitatMPCL, data = habitatN, family = binomial("cloglog"))
plot(mod_ECBBJCNF_MPCL.mvb)
anova(mod_BBCLECJCMPNF.mvb, mod_ECBBJCNF_MPCL.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  # p = 0.001, df = 1, score= 71.41, nBoot = 999, 20 mins.  CL and MP are sig diff


# do a table-wide correction, under the assumption that there are 30 possible pairwise comparisons between all 6 forest types:  (5*6)/2 = 15
pvalues <- c(0.001, .001, .001, .001)
pvalues.corr.fdr<-p.adjust(pvalues, method = "fdr", n = length(pvalues)) 
pvalues.corr.fdr 
# [1] 0.001 0.001 0.001 0.001

# pvalues.corr.fdr<-p.adjust(pvalues, method = "fdr", n = length(pvalues))
```

# we also need a new mvabund analysis:  elevation + habitat + elevation*habitat
# if the interaction is not sig, then reduce to elevation + habitat and run again
```{r mvabud test for ele_hab}

communityB.mvb <- mvabund(communityB) 
#nboot <- 499 # set to 20 for debugging (~25 mins for nboot = 499)
nboot <- 999 # set to 999 for publication

# base model with Elevation_m+Habitat+Elevation_m*Habitat
mod_ele_hab_x.mvb <- manyglm(communityB.mvb ~ Elevation_m+Habitat+Elevation_m*Habitat, data = habitatN, family = binomial("cloglog"))

plot(mod_ele_hab_x.mvb)
anova(mod_ele_hab_x.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  
# Model: manyglm(formula = communityB.mvb ~ Elevation_m + Habitat + Elevation_m * 
# Model:     Habitat, family = binomial("cloglog"), data = habitatN)
# 
# Multivariate test:
#                     Res.Df Df.diff score Pr(>score)    
# (Intercept)             60                             
# Elevation_m             59       1 195.8      0.001 ***
# Habitat                 54       5 829.0      0.001 ***
# Elevation_m:Habitat     49       5 230.0      0.001 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

#mod_ele_hab.mvb <- manyglm(communityB.mvb ~ Elevation_m+Habitat, data = habitatN, family = binomial("cloglog"))

#plot(mod_ele_hab.mvb)
#anova(mod_ele_hab.mvb, cor.type = "shrink", test = "score", show.time = "all", nBoot = nboot)  
# Model: manyglm(formula = communityB.mvb ~ Elevation_m + Habitat, family = binomial("cloglog"), 
# Model:     data = habitatN)
# 
# Multivariate test:
#             Res.Df Df.diff score Pr(>score)    
# (Intercept)     60                             
# Elevation_m     59       1 116.4      0.001 ***
# Habitat         54       5 557.4      0.001 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

```



